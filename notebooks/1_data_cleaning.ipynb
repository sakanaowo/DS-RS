{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de99501",
   "metadata": {},
   "source": [
    "# 1. Data cleaning and exploration\n",
    "\n",
    "Use these cells to scan everything under `data/archive/` so we know what to clean and how to shape a loader later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4e87",
   "metadata": {},
   "source": [
    "## Setup and helpers\n",
    "\n",
    "- Reads each CSV with a row cap to stay memory-safe.\n",
    "- Shows basic column profile (dtype, null %, unique counts).\n",
    "- Plots a quick numeric histogram and top categories for the first available categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfba789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "archive_dir = Path('../data/archive')\n",
    "sample_rows = 150_000  # adjust if you need more/less for big files\n",
    "\n",
    "def read_csv_sample(path: Path, nrows: int = sample_rows) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV with a row limit to avoid blowing up memory.\"\"\"\n",
    "    return pd.read_csv(path, nrows=nrows)\n",
    "\n",
    "def profile(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    meta = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'non_null': df.notna().sum(),\n",
    "        'null_pct': (df.isna().mean() * 100).round(2),\n",
    "        'unique': df.nunique()\n",
    "    })\n",
    "    return meta.sort_values('null_pct', ascending=False)\n",
    "\n",
    "def plot_quick(df: pd.DataFrame, title: str) -> None:\n",
    "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    cat_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
    "\n",
    "    if num_cols:\n",
    "        col = num_cols[0]\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        sns.histplot(df[col].dropna(), bins=40)\n",
    "        plt.title(f'{title} - {col}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if cat_cols:\n",
    "        col = cat_cols[0]\n",
    "        top = df[col].value_counts().head(15)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.barplot(x=top.values, y=top.index, orient='h')\n",
    "        plt.title(f'{title} - top {col}')\n",
    "        plt.xlabel('count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f52e7",
   "metadata": {},
   "source": [
    "## List archive files\n",
    "\n",
    "Quick inventory with file sizes to gauge what to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa49ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 CSV files under ..\\data\\archive\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to WindowsPath.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[0;32m      4\u001b[0m     size_mb \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstat()\u001b[38;5;241m.\u001b[39mst_size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;241m.\u001b[39mrelative_to(archive_dir)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m35\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_mb\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to WindowsPath.__format__"
     ]
    }
   ],
   "source": [
    "csv_files = sorted(archive_dir.rglob('*.csv'))\n",
    "print(f'Found {len(csv_files)} CSV files under {archive_dir}')\n",
    "for p in csv_files:\n",
    "    size_mb = p.stat().st_size / 1e6\n",
    "    rel = str(p.relative_to(archive_dir))\n",
    "    print(f'{rel:35} {size_mb:8.1f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f6679",
   "metadata": {},
   "source": [
    "## Per-file quick profile\n",
    "\n",
    "Loops through every CSV: sample rows, preview head, column profile, and quick plots. Increase `sample_rows` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in csv_files:\n",
    "    print(f\"\\n=== {p.relative_to(archive_dir)} ===\")\n",
    "    df = read_csv_sample(p)\n",
    "    print('shape (sample):', df.shape)\n",
    "    display(df.head())\n",
    "    display(profile(df))\n",
    "    plot_quick(df, p.stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165666c4",
   "metadata": {},
   "source": [
    "## Key tables to feed loader.py\n",
    "\n",
    "Load samples of the main tables we will likely join (`postings`, `companies`, `jobs` components, `mappings`) to inspect column names and keys for cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_paths = {\n",
    "    'postings': archive_dir / 'postings.csv',\n",
    "    'companies': archive_dir / 'companies/companies.csv',\n",
    "    'company_industries': archive_dir / 'companies/company_industries.csv',\n",
    "    'company_specialities': archive_dir / 'companies/company_specialities.csv',\n",
    "    'jobs_benefits': archive_dir / 'jobs/benefits.csv',\n",
    "    'jobs_industries': archive_dir / 'jobs/job_industries.csv',\n",
    "    'jobs_skills': archive_dir / 'jobs/job_skills.csv',\n",
    "    'jobs_salaries': archive_dir / 'jobs/salaries.csv',\n",
    "    'map_industries': archive_dir / 'mappings/industries.csv',\n",
    "    'map_skills': archive_dir / 'mappings/skills.csv',\n",
    "}\n",
    "\n",
    "frames = {}\n",
    "for name, path in key_paths.items():\n",
    "    if not path.exists():\n",
    "        print(f'Missing: {name} at {path}')\n",
    "        continue\n",
    "    nrows = sample_rows if name == 'postings' else min(sample_rows, 50_000)\n",
    "    frames[name] = read_csv_sample(path, nrows=nrows)\n",
    "    print(f'{name}: {frames[name].shape} from {path.name}')\n",
    "    display(frames[name].head())\n",
    "    display(profile(frames[name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd8bf0",
   "metadata": {},
   "source": [
    "## Next steps for cleaning\n",
    "\n",
    "- Decide join keys (e.g., `job_id`, `company_id`) from the previews above.\n",
    "- Drop or fill high-null columns; deduplicate by ids.\n",
    "- Standardize text fields (`job_title`, `description`, `skills`) for vectorization.\n",
    "- Save a cleaned, joined table to `data/processed/clean_jobs.csv` for use in `src/loader.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
