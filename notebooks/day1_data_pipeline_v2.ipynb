{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcad3ae7",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade58743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Project root: /home/sakana/Code/DS-RS\n",
      "‚úì Raw data: /home/sakana/Code/DS-RS/data/raw\n",
      "‚úì Processed data: /home/sakana/Code/DS-RS/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "print(f\"‚úì Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úì Raw data: {RAW_DIR}\")\n",
    "print(f\"‚úì Processed data: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb4e6c",
   "metadata": {},
   "source": [
    "## Part 2: Helper Functions\n",
    "\n",
    "### 2.1 Parse Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aeac309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parse_location():\n",
      "  'San Francisco, CA, United States'       ‚Üí {'city': 'San Francisco', 'state': 'CA', 'country': 'United States'}\n",
      "  'New York, NY'                           ‚Üí {'city': 'New York', 'state': 'NY', 'country': 'United States'}\n",
      "  'Remote'                                 ‚Üí {'city': 'Remote', 'state': '', 'country': ''}\n",
      "  'United States'                          ‚Üí {'city': '', 'state': '', 'country': 'United States'}\n",
      "  'London, United Kingdom'                 ‚Üí {'city': 'London', 'state': '', 'country': 'United Kingdom'}\n",
      "  None                                     ‚Üí {'city': '', 'state': '', 'country': ''}\n",
      "  ''                                       ‚Üí {'city': '', 'state': '', 'country': ''}\n"
     ]
    }
   ],
   "source": [
    "def parse_location(loc_str: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse location string into city, state, country.\n",
    "    \n",
    "    Examples:\n",
    "    - \"San Francisco, CA, United States\" ‚Üí {'city': 'San Francisco', 'state': 'CA', 'country': 'United States'}\n",
    "    - \"New York, NY\" ‚Üí {'city': 'New York', 'state': 'NY', 'country': 'United States'}\n",
    "    - \"Remote\" ‚Üí {'city': 'Remote', 'state': '', 'country': ''}\n",
    "    - \"United States\" ‚Üí {'city': '', 'state': '', 'country': 'United States'}\n",
    "    \"\"\"\n",
    "    if pd.isna(loc_str) or not isinstance(loc_str, str) or loc_str.strip() == '':\n",
    "        return {'city': '', 'state': '', 'country': ''}\n",
    "    \n",
    "    location = loc_str.strip()\n",
    "    \n",
    "    # Special cases\n",
    "    if location.lower() == 'remote':\n",
    "        return {'city': 'Remote', 'state': '', 'country': ''}\n",
    "    \n",
    "    if location == 'United States':\n",
    "        return {'city': '', 'state': '', 'country': 'United States'}\n",
    "    \n",
    "    # Split by comma\n",
    "    parts = [p.strip() for p in location.split(',')]\n",
    "    \n",
    "    if len(parts) == 1:\n",
    "        # Only one part - could be city or country\n",
    "        return {'city': parts[0], 'state': '', 'country': ''}\n",
    "    \n",
    "    elif len(parts) == 2:\n",
    "        # Two parts - city, state OR city, country\n",
    "        city, second = parts\n",
    "        # If second part is 2 uppercase letters, likely US state\n",
    "        if len(second) == 2 and second.isupper():\n",
    "            return {'city': city, 'state': second, 'country': 'United States'}\n",
    "        else:\n",
    "            return {'city': city, 'state': '', 'country': second}\n",
    "    \n",
    "    else:\n",
    "        # Three or more parts - city, state, country\n",
    "        return {\n",
    "            'city': parts[0],\n",
    "            'state': parts[1],\n",
    "            'country': parts[-1]\n",
    "        }\n",
    "\n",
    "# Test\n",
    "test_cases = [\n",
    "    \"San Francisco, CA, United States\",\n",
    "    \"New York, NY\",\n",
    "    \"Remote\",\n",
    "    \"United States\",\n",
    "    \"London, United Kingdom\",\n",
    "    None,\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "print(\"Testing parse_location():\")\n",
    "for test in test_cases:\n",
    "    result = parse_location(test)\n",
    "    print(f\"  {test!r:40s} ‚Üí {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277eec6",
   "metadata": {},
   "source": [
    "### 2.2 Normalize Salary to Yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5cc6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalize_salary_to_yearly():\n",
      "  80000.0-120000 YEARLY     ‚Üí $100,000\n",
      "  25.0-35 HOURLY     ‚Üí $62,400\n",
      "  5000.0-7000 MONTHLY    ‚Üí $72,000\n",
      "  nan-100000 YEARLY     ‚Üí None\n"
     ]
    }
   ],
   "source": [
    "def normalize_salary_to_yearly(row: pd.Series) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert salary to yearly amount.\n",
    "    \n",
    "    Args:\n",
    "        row: pandas Series with columns: min_salary, max_salary, pay_period\n",
    "    \n",
    "    Returns:\n",
    "        Yearly salary (median of min and max), or None if missing\n",
    "    \n",
    "    Multipliers:\n",
    "    - YEARLY: 1\n",
    "    - MONTHLY: 12\n",
    "    - BIWEEKLY: 26\n",
    "    - WEEKLY: 52\n",
    "    - HOURLY: 2080 (40 hours/week √ó 52 weeks)\n",
    "    \"\"\"\n",
    "    # Check if salary data exists\n",
    "    if pd.isna(row.get('min_salary')) or pd.isna(row.get('max_salary')):\n",
    "        return None\n",
    "    \n",
    "    # Calculate median\n",
    "    try:\n",
    "        min_sal = float(row['min_salary'])\n",
    "        max_sal = float(row['max_salary'])\n",
    "        median = (min_sal + max_sal) / 2\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "    # Get pay period\n",
    "    period = str(row.get('pay_period', '')).upper()\n",
    "    \n",
    "    # Conversion multipliers\n",
    "    multipliers = {\n",
    "        'YEARLY': 1,\n",
    "        'MONTHLY': 12,\n",
    "        'BIWEEKLY': 26,\n",
    "        'WEEKLY': 52,\n",
    "        'HOURLY': 2080,  # 40h/week √ó 52 weeks\n",
    "    }\n",
    "    \n",
    "    multiplier = multipliers.get(period, 1)\n",
    "    return median * multiplier\n",
    "\n",
    "# Test\n",
    "test_data = pd.DataFrame([\n",
    "    {'min_salary': 80000, 'max_salary': 120000, 'pay_period': 'YEARLY'},\n",
    "    {'min_salary': 25, 'max_salary': 35, 'pay_period': 'HOURLY'},\n",
    "    {'min_salary': 5000, 'max_salary': 7000, 'pay_period': 'MONTHLY'},\n",
    "    {'min_salary': None, 'max_salary': 100000, 'pay_period': 'YEARLY'},\n",
    "])\n",
    "\n",
    "print(\"Testing normalize_salary_to_yearly():\")\n",
    "for idx, row in test_data.iterrows():\n",
    "    result = normalize_salary_to_yearly(row)\n",
    "    print(f\"  {row['min_salary']}-{row['max_salary']} {row['pay_period']:10s} ‚Üí ${result:,.0f}\" if result else f\"  {row['min_salary']}-{row['max_salary']} {row['pay_period']:10s} ‚Üí None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9a7b2",
   "metadata": {},
   "source": [
    "## Part 3: Data Loading Functions\n",
    "\n",
    "### 3.1 Load Jobs (Normalized - NO Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71cf62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with sample (1000 rows)...\n",
      "\n",
      "============================================================\n",
      "LOADING JOBS (Normalized)\n",
      "============================================================\n",
      "Step 1: Reading postings.csv...\n",
      "  ‚úì Loaded 1,000 rows\n",
      "\n",
      "Step 2: Selecting columns...\n",
      "\n",
      "Step 3: Cleaning data...\n",
      "  ‚úì Dropped 0 rows with missing title/description\n",
      "  ‚úì Dropped 0 duplicate job_id rows\n",
      "\n",
      "Step 4: Parsing location...\n",
      "  ‚úì Parsed into city, state, country\n",
      "\n",
      "Step 5: Normalizing salary to yearly...\n",
      "  ‚úì 338 jobs have salary data (33.8%)\n",
      "\n",
      "Step 6: Converting data types...\n",
      "  ‚úì Converted dtypes\n",
      "\n",
      "‚úì Final shape: (1000, 20)\n",
      "‚úì Memory usage: 5.4 MB\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "job_id",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "work_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "235bfe59-d209-40f7-97ce-ef8a89290a17",
       "rows": [
        [
         "0",
         "921716",
         "Marketing Coordinator",
         "Princeton",
         "NJ",
         "United States",
         "Full-time"
        ],
        [
         "1",
         "1829192",
         "Mental Health Therapist/Counselor",
         "Fort Collins",
         "CO",
         "United States",
         "Full-time"
        ],
        [
         "2",
         "10998357",
         "Assitant Restaurant Manager",
         "Cincinnati",
         "OH",
         "United States",
         "Full-time"
        ],
        [
         "3",
         "23221523",
         "Senior Elder Law / Trusts and Estates Associate Attorney",
         "New Hyde Park",
         "NY",
         "United States",
         "Full-time"
        ],
        [
         "4",
         "35982263",
         " Service Technician",
         "Burlington",
         "IA",
         "United States",
         "Full-time"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>work_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921716</td>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>NJ</td>\n",
       "      <td>United States</td>\n",
       "      <td>Full-time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1829192</td>\n",
       "      <td>Mental Health Therapist/Counselor</td>\n",
       "      <td>Fort Collins</td>\n",
       "      <td>CO</td>\n",
       "      <td>United States</td>\n",
       "      <td>Full-time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10998357</td>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>OH</td>\n",
       "      <td>United States</td>\n",
       "      <td>Full-time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23221523</td>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>New Hyde Park</td>\n",
       "      <td>NY</td>\n",
       "      <td>United States</td>\n",
       "      <td>Full-time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35982263</td>\n",
       "      <td>Service Technician</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>IA</td>\n",
       "      <td>United States</td>\n",
       "      <td>Full-time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_id                                              title           city  \\\n",
       "0    921716                              Marketing Coordinator      Princeton   \n",
       "1   1829192                  Mental Health Therapist/Counselor   Fort Collins   \n",
       "2  10998357                        Assitant Restaurant Manager     Cincinnati   \n",
       "3  23221523  Senior Elder Law / Trusts and Estates Associat...  New Hyde Park   \n",
       "4  35982263                                 Service Technician     Burlington   \n",
       "\n",
       "  state        country  work_type  \n",
       "0    NJ  United States  Full-time  \n",
       "1    CO  United States  Full-time  \n",
       "2    OH  United States  Full-time  \n",
       "3    NY  United States  Full-time  \n",
       "4    IA  United States  Full-time  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jobs_normalized(sample: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load jobs table without aggregation.\n",
    "    \n",
    "    Args:\n",
    "        sample: If not None, only load first N rows (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns:\n",
    "        - job_id, title, description, company_id, company_name\n",
    "        - location, city, state, country\n",
    "        - work_type, experience_level, remote_allowed\n",
    "        - min_salary, max_salary, pay_period, normalized_salary_yearly\n",
    "        - views, applies, listed_time, closed_time\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING JOBS (Normalized)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load raw postings\n",
    "    print(\"Step 1: Reading postings.csv...\")\n",
    "    postings_path = RAW_DIR / 'postings.csv'\n",
    "    \n",
    "    nrows = sample if sample else None\n",
    "    postings = pd.read_csv(postings_path, nrows=nrows)\n",
    "    print(f\"  ‚úì Loaded {len(postings):,} rows\")\n",
    "    \n",
    "    # 2. Select columns\n",
    "    print(\"\\nStep 2: Selecting columns...\")\n",
    "    jobs = postings[[\n",
    "        'job_id', 'title', 'description', 'company_id', 'company_name',\n",
    "        'location', 'formatted_work_type', 'formatted_experience_level',\n",
    "        'remote_allowed', 'min_salary', 'max_salary', 'pay_period',\n",
    "        'views', 'applies', 'original_listed_time', 'closed_time'\n",
    "    ]].copy()\n",
    "    \n",
    "    # 3. Clean\n",
    "    print(\"\\nStep 3: Cleaning data...\")\n",
    "    \n",
    "    # Drop jobs without title or description\n",
    "    before = len(jobs)\n",
    "    jobs = jobs[jobs['title'].notna() & (jobs['title'].str.strip() != '')]\n",
    "    jobs = jobs[jobs['description'].notna() & (jobs['description'].str.strip() != '')]\n",
    "    print(f\"  ‚úì Dropped {before - len(jobs):,} rows with missing title/description\")\n",
    "    \n",
    "    # Remove duplicates by job_id\n",
    "    before = len(jobs)\n",
    "    jobs = jobs.drop_duplicates(subset=['job_id'], keep='first')\n",
    "    print(f\"  ‚úì Dropped {before - len(jobs):,} duplicate job_id rows\")\n",
    "    \n",
    "    # 4. Parse location\n",
    "    print(\"\\nStep 4: Parsing location...\")\n",
    "    location_parsed = jobs['location'].fillna('').apply(parse_location)\n",
    "    jobs['city'] = location_parsed.apply(lambda x: x['city'])\n",
    "    jobs['state'] = location_parsed.apply(lambda x: x['state'])\n",
    "    jobs['country'] = location_parsed.apply(lambda x: x['country'])\n",
    "    print(f\"  ‚úì Parsed into city, state, country\")\n",
    "    \n",
    "    # 5. Normalize salary\n",
    "    print(\"\\nStep 5: Normalizing salary to yearly...\")\n",
    "    jobs['normalized_salary_yearly'] = jobs.apply(normalize_salary_to_yearly, axis=1)\n",
    "    salary_count = jobs['normalized_salary_yearly'].notna().sum()\n",
    "    print(f\"  ‚úì {salary_count:,} jobs have salary data ({salary_count/len(jobs)*100:.1f}%)\")\n",
    "    \n",
    "    # 6. Rename columns\n",
    "    jobs = jobs.rename(columns={\n",
    "        'formatted_work_type': 'work_type',\n",
    "        'formatted_experience_level': 'experience_level',\n",
    "        'original_listed_time': 'listed_time'\n",
    "    })\n",
    "    \n",
    "    # 7. Convert dtypes\n",
    "    print(\"\\nStep 6: Converting data types...\")\n",
    "    jobs['job_id'] = pd.to_numeric(jobs['job_id'], errors='coerce').astype('Int64')\n",
    "    jobs['company_id'] = pd.to_numeric(jobs['company_id'], errors='coerce').astype('Int64')\n",
    "    jobs['remote_allowed'] = jobs['remote_allowed'].astype('boolean')\n",
    "    jobs['listed_time'] = pd.to_datetime(jobs['listed_time'], errors='coerce')\n",
    "    jobs['closed_time'] = pd.to_datetime(jobs['closed_time'], errors='coerce')\n",
    "    print(f\"  ‚úì Converted dtypes\")\n",
    "    \n",
    "    print(f\"\\n‚úì Final shape: {jobs.shape}\")\n",
    "    print(f\"‚úì Memory usage: {jobs.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Test with sample\n",
    "print(\"Testing with sample (1000 rows)...\")\n",
    "jobs_sample = load_jobs_normalized(sample=1000)\n",
    "print(\"\\nSample data:\")\n",
    "jobs_sample[['job_id', 'title', 'city', 'state', 'country', 'work_type']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb156e",
   "metadata": {},
   "source": [
    "### 3.2 Load Job Skills (Many-to-Many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533c8e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING JOB_SKILLS (Many-to-Many)\n",
      "============================================================\n",
      "‚úì Loaded 213,768 job-skill relationships\n",
      "‚úì Dropped 0 rows with missing values\n",
      "‚úì Memory usage: 12.5 MB\n",
      "\n",
      "Sample data:\n",
      "       job_id skill_abr\n",
      "0  3884428798      MRKT\n",
      "1  3884428798        PR\n",
      "2  3884428798       WRT\n",
      "3  3887473071      SALE\n",
      "4  3887465684       FIN\n",
      "5  3887465684      SALE\n",
      "6  3887467939      SALE\n",
      "7  3887467939      ADVR\n",
      "8  3887467939        BD\n",
      "9  3887471331       ENG\n",
      "\n",
      "Unique jobs: 126,807\n",
      "Unique skills: 35\n"
     ]
    }
   ],
   "source": [
    "def load_job_skills() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load job-skill relationships (NO aggregation).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: job_id, skill_abr\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING JOB_SKILLS (Many-to-Many)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    path = RAW_DIR / 'jobs' / 'job_skills.csv'\n",
    "    job_skills = pd.read_csv(path)\n",
    "    \n",
    "    # Keep only needed columns\n",
    "    job_skills = job_skills[['job_id', 'skill_abr']].copy()\n",
    "    \n",
    "    # Convert dtypes\n",
    "    job_skills['job_id'] = pd.to_numeric(job_skills['job_id'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    before = len(job_skills)\n",
    "    job_skills = job_skills.dropna()\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(job_skills):,} job-skill relationships\")\n",
    "    print(f\"‚úì Dropped {before - len(job_skills):,} rows with missing values\")\n",
    "    print(f\"‚úì Memory usage: {job_skills.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return job_skills\n",
    "\n",
    "# Test\n",
    "job_skills = load_job_skills()\n",
    "print(\"\\nSample data:\")\n",
    "print(job_skills.head(10))\n",
    "print(f\"\\nUnique jobs: {job_skills['job_id'].nunique():,}\")\n",
    "print(f\"Unique skills: {job_skills['skill_abr'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc975",
   "metadata": {},
   "source": [
    "### 3.3 Load Skills Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b226ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING SKILLS (Lookup Table)\n",
      "============================================================\n",
      "‚úì Loaded 35 skills\n",
      "\n",
      "All skills:\n",
      "   skill_abr              skill_name\n",
      "0        ART            Art/Creative\n",
      "1       DSGN                  Design\n",
      "2       ADVR             Advertising\n",
      "3       PRDM      Product Management\n",
      "4       DIST            Distribution\n",
      "5        EDU               Education\n",
      "6       TRNG                Training\n",
      "7       PRJM      Project Management\n",
      "8       CNSL              Consulting\n",
      "9       PRCH              Purchasing\n",
      "10      SUPL            Supply Chain\n",
      "11      ANLS                 Analyst\n",
      "12      HCPR    Health Care Provider\n",
      "13      RSCH                Research\n",
      "14       SCI                 Science\n",
      "15      GENB        General Business\n",
      "16      CUST        Customer Service\n",
      "17      STRA       Strategy/Planning\n",
      "18       FIN                 Finance\n",
      "19      OTHR                   Other\n",
      "20       LGL                   Legal\n",
      "21       ENG             Engineering\n",
      "22        QA       Quality Assurance\n",
      "23        BD    Business Development\n",
      "24        IT  Information Technology\n",
      "25       ADM          Administrative\n",
      "26      PROD              Production\n",
      "27      MRKT               Marketing\n",
      "28        PR        Public Relations\n",
      "29       WRT         Writing/Editing\n",
      "30      ACCT     Accounting/Auditing\n",
      "31        HR         Human Resources\n",
      "32      MNFC           Manufacturing\n",
      "33      SALE                   Sales\n",
      "34      MGMT              Management\n"
     ]
    }
   ],
   "source": [
    "def load_skills() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load skills lookup table.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: skill_abr, skill_name\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING SKILLS (Lookup Table)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    path = RAW_DIR / 'mappings' / 'skills.csv'\n",
    "    skills = pd.read_csv(path)\n",
    "    skills = skills[['skill_abr', 'skill_name']].copy()\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(skills):,} skills\")\n",
    "    \n",
    "    return skills\n",
    "\n",
    "# Test\n",
    "skills = load_skills()\n",
    "print(\"\\nAll skills:\")\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a7340",
   "metadata": {},
   "source": [
    "### 3.4 Load Job Industries (Many-to-Many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83afb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING JOB_INDUSTRIES (Many-to-Many)\n",
      "============================================================\n",
      "‚úì Loaded 164,808 job-industry relationships\n",
      "‚úì Dropped 0 rows with missing values\n",
      "‚úì Memory usage: 2.8 MB\n",
      "\n",
      "Sample data:\n",
      "       job_id  industry_id\n",
      "0  3884428798           82\n",
      "1  3887473071           48\n",
      "2  3887465684           41\n",
      "3  3887467939           82\n",
      "4  3887467939           80\n",
      "5  3887471331           57\n",
      "6  3887471331          332\n",
      "7  3887471331          383\n",
      "8  3887471274           82\n",
      "9  3887471274           80\n",
      "\n",
      "Unique jobs: 127,125\n",
      "Unique industries: 422\n"
     ]
    }
   ],
   "source": [
    "def load_job_industries() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load job-industry relationships (NO aggregation).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: job_id, industry_id\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING JOB_INDUSTRIES (Many-to-Many)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    path = RAW_DIR / 'jobs' / 'job_industries.csv'\n",
    "    job_industries = pd.read_csv(path)\n",
    "    \n",
    "    # Keep only needed columns\n",
    "    job_industries = job_industries[['job_id', 'industry_id']].copy()\n",
    "    \n",
    "    # Convert dtypes\n",
    "    job_industries['job_id'] = pd.to_numeric(job_industries['job_id'], errors='coerce').astype('Int64')\n",
    "    job_industries['industry_id'] = pd.to_numeric(job_industries['industry_id'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    before = len(job_industries)\n",
    "    job_industries = job_industries.dropna()\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(job_industries):,} job-industry relationships\")\n",
    "    print(f\"‚úì Dropped {before - len(job_industries):,} rows with missing values\")\n",
    "    print(f\"‚úì Memory usage: {job_industries.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return job_industries\n",
    "\n",
    "# Test\n",
    "job_industries = load_job_industries()\n",
    "print(\"\\nSample data:\")\n",
    "print(job_industries.head(10))\n",
    "print(f\"\\nUnique jobs: {job_industries['job_id'].nunique():,}\")\n",
    "print(f\"Unique industries: {job_industries['industry_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ad5ed",
   "metadata": {},
   "source": [
    "### 3.5 Load Industries Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b3437b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING INDUSTRIES (Lookup Table)\n",
      "============================================================\n",
      "‚úì Loaded 422 industries\n",
      "\n",
      "Sample industries:\n",
      "    industry_id                         industry_name\n",
      "0             1       Defense and Space Manufacturing\n",
      "1             3       Computer Hardware Manufacturing\n",
      "2             4                  Software Development\n",
      "3             5          Computer Networking Products\n",
      "4             6  Technology, Information and Internet\n",
      "5             7           Semiconductor Manufacturing\n",
      "6             8                    Telecommunications\n",
      "7             9                          Law Practice\n",
      "8            10                        Legal Services\n",
      "9            11      Business Consulting and Services\n",
      "10           12                Biotechnology Research\n",
      "11           13                     Medical Practices\n",
      "12           14             Hospitals and Health Care\n",
      "13           15          Pharmaceutical Manufacturing\n",
      "14           16                   Veterinary Services\n",
      "15           17       Medical Equipment Manufacturing\n",
      "16           18   Personal Care Product Manufacturing\n",
      "17           19            Retail Apparel and Fashion\n",
      "18           20          Sporting Goods Manufacturing\n",
      "19           21                 Tobacco Manufacturing\n"
     ]
    }
   ],
   "source": [
    "def load_industries() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load industries lookup table.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: industry_id, industry_name\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING INDUSTRIES (Lookup Table)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    path = RAW_DIR / 'mappings' / 'industries.csv'\n",
    "    industries = pd.read_csv(path)\n",
    "    industries = industries[['industry_id', 'industry_name']].copy()\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(industries):,} industries\")\n",
    "    \n",
    "    return industries\n",
    "\n",
    "# Test\n",
    "industries = load_industries()\n",
    "print(\"\\nSample industries:\")\n",
    "print(industries.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b21f1",
   "metadata": {},
   "source": [
    "## Part 4: Verification v·ªõi Sample Data\n",
    "\n",
    "### 4.1 Test JOIN logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf33ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing v·ªõi job_id = 921716\n",
      "\n",
      "============================================================\n",
      "Job: Marketing Coordinator\n",
      "Company: Corcoran Sawyer Smith\n",
      "Location: Princeton, NJ\n",
      "\n",
      "Skills (2 skills):\n",
      "  - MRKT: Marketing\n",
      "  - SALE: Sales\n",
      "\n",
      "Industries (1 industries):\n",
      "  - 44: Real Estate\n",
      "\n",
      "============================================================\n",
      "‚úì JOIN logic ho·∫°t ƒë·ªông ƒë√∫ng!\n",
      "‚úì Skills v√† Industries KH√îNG b·ªã aggregate th√†nh string\n"
     ]
    }
   ],
   "source": [
    "# Test v·ªõi 1 job c·ª• th·ªÉ\n",
    "test_job_id = jobs_sample['job_id'].iloc[0]\n",
    "\n",
    "print(f\"Testing v·ªõi job_id = {test_job_id}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Job info\n",
    "job_info = jobs_sample[jobs_sample['job_id'] == test_job_id][['title', 'company_name', 'city', 'state']].iloc[0]\n",
    "print(f\"Job: {job_info['title']}\")\n",
    "print(f\"Company: {job_info['company_name']}\")\n",
    "print(f\"Location: {job_info['city']}, {job_info['state']}\")\n",
    "\n",
    "# Skills cho job n√†y\n",
    "job_skill_list = job_skills[job_skills['job_id'] == test_job_id]\n",
    "print(f\"\\nSkills ({len(job_skill_list)} skills):\")\n",
    "for _, row in job_skill_list.iterrows():\n",
    "    skill_name = skills[skills['skill_abr'] == row['skill_abr']]['skill_name'].values\n",
    "    skill_name = skill_name[0] if len(skill_name) > 0 else 'Unknown'\n",
    "    print(f\"  - {row['skill_abr']}: {skill_name}\")\n",
    "\n",
    "# Industries cho job n√†y\n",
    "job_industry_list = job_industries[job_industries['job_id'] == test_job_id]\n",
    "print(f\"\\nIndustries ({len(job_industry_list)} industries):\")\n",
    "for _, row in job_industry_list.iterrows():\n",
    "    industry_name = industries[industries['industry_id'] == row['industry_id']]['industry_name'].values\n",
    "    industry_name = industry_name[0] if len(industry_name) > 0 else 'Unknown'\n",
    "    print(f\"  - {row['industry_id']}: {industry_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì JOIN logic ho·∫°t ƒë·ªông ƒë√∫ng!\")\n",
    "print(\"‚úì Skills v√† Industries KH√îNG b·ªã aggregate th√†nh string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab2ae3",
   "metadata": {},
   "source": [
    "### 4.2 Verify Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebd9a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "1. JOBS TABLE\n",
      "   ‚úì Rows: 1,000\n",
      "   ‚úì Unique job_id: True\n",
      "   ‚úì No missing title: True\n",
      "   ‚úì No missing description: True\n",
      "   ‚úì Salary coverage: 33.8%\n",
      "\n",
      "2. JOB_SKILLS TABLE\n",
      "   ‚úì Rows: 213,768\n",
      "   ‚úì Unique jobs: 126,807\n",
      "   ‚úì Unique skills: 35\n",
      "   ‚úì Avg skills per job: 1.69\n",
      "\n",
      "3. SKILLS LOOKUP\n",
      "   ‚úì Total skills: 35\n",
      "\n",
      "4. JOB_INDUSTRIES TABLE\n",
      "   ‚úì Rows: 164,808\n",
      "   ‚úì Unique jobs: 127,125\n",
      "   ‚úì Unique industries: 422\n",
      "   ‚úì Avg industries per job: 1.30\n",
      "\n",
      "5. INDUSTRIES LOOKUP\n",
      "   ‚úì Total industries: 422\n",
      "\n",
      "6. FOREIGN KEY INTEGRITY\n",
      "   ‚úì Invalid skills in job_skills: 0\n",
      "   ‚úì Invalid industries in job_industries: 0\n",
      "\n",
      "============================================================\n",
      "‚úì ALL CHECKS PASSED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def verify_data_quality(jobs, job_skills, skills, job_industries, industries):\n",
    "    \"\"\"Verify data quality c·ªßa normalized tables.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY VERIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Jobs table\n",
    "    print(\"\\n1. JOBS TABLE\")\n",
    "    print(f\"   ‚úì Rows: {len(jobs):,}\")\n",
    "    print(f\"   ‚úì Unique job_id: {jobs['job_id'].is_unique}\")\n",
    "    print(f\"   ‚úì No missing title: {jobs['title'].notna().all()}\")\n",
    "    print(f\"   ‚úì No missing description: {jobs['description'].notna().all()}\")\n",
    "    print(f\"   ‚úì Salary coverage: {jobs['normalized_salary_yearly'].notna().sum()/len(jobs)*100:.1f}%\")\n",
    "    \n",
    "    # 2. Job Skills\n",
    "    print(\"\\n2. JOB_SKILLS TABLE\")\n",
    "    print(f\"   ‚úì Rows: {len(job_skills):,}\")\n",
    "    print(f\"   ‚úì Unique jobs: {job_skills['job_id'].nunique():,}\")\n",
    "    print(f\"   ‚úì Unique skills: {job_skills['skill_abr'].nunique():,}\")\n",
    "    print(f\"   ‚úì Avg skills per job: {len(job_skills)/job_skills['job_id'].nunique():.2f}\")\n",
    "    \n",
    "    # 3. Skills lookup\n",
    "    print(\"\\n3. SKILLS LOOKUP\")\n",
    "    print(f\"   ‚úì Total skills: {len(skills):,}\")\n",
    "    \n",
    "    # 4. Job Industries\n",
    "    print(\"\\n4. JOB_INDUSTRIES TABLE\")\n",
    "    print(f\"   ‚úì Rows: {len(job_industries):,}\")\n",
    "    print(f\"   ‚úì Unique jobs: {job_industries['job_id'].nunique():,}\")\n",
    "    print(f\"   ‚úì Unique industries: {job_industries['industry_id'].nunique():,}\")\n",
    "    print(f\"   ‚úì Avg industries per job: {len(job_industries)/job_industries['job_id'].nunique():.2f}\")\n",
    "    \n",
    "    # 5. Industries lookup\n",
    "    print(\"\\n5. INDUSTRIES LOOKUP\")\n",
    "    print(f\"   ‚úì Total industries: {len(industries):,}\")\n",
    "    \n",
    "    # 6. Foreign key integrity\n",
    "    print(\"\\n6. FOREIGN KEY INTEGRITY\")\n",
    "    \n",
    "    # Check job_skills references valid skills\n",
    "    invalid_skills = set(job_skills['skill_abr']) - set(skills['skill_abr'])\n",
    "    print(f\"   ‚úì Invalid skills in job_skills: {len(invalid_skills)}\")\n",
    "    \n",
    "    # Check job_industries references valid industries\n",
    "    invalid_industries = set(job_industries['industry_id']) - set(industries['industry_id'])\n",
    "    print(f\"   ‚úì Invalid industries in job_industries: {len(invalid_industries)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úì ALL CHECKS PASSED!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run verification v·ªõi sample data\n",
    "verify_data_quality(jobs_sample, job_skills, skills, job_industries, industries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74863311",
   "metadata": {},
   "source": [
    "## Part 5: Save Functions\n",
    "\n",
    "### 5.1 Save Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2c46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing save function v·ªõi sample data...\n",
      "\n",
      "============================================================\n",
      "SAVING NORMALIZED DATA\n",
      "============================================================\n",
      "  ‚úì jobs.parquet                 1.6 MB  (1,000 rows)\n",
      "  ‚úì job_skills.parquet           1.1 MB  (213,768 rows)\n",
      "  ‚úì skills.parquet               0.0 MB  (35 rows)\n",
      "  ‚úì job_industries.parquet       1.0 MB  (164,808 rows)\n",
      "  ‚úì industries.parquet           0.0 MB  (422 rows)\n",
      "\n",
      "  Total: 3.8 MB\n",
      "\n",
      "‚úì All files saved to: /home/sakana/Code/DS-RS/data/processed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def save_normalized_data(jobs, job_skills, skills, job_industries, industries):\n",
    "    \"\"\"\n",
    "    Save all normalized tables to data/processed/\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING NORMALIZED DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create directory if not exists\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save as Parquet (compact, fast)\n",
    "    files = {\n",
    "        'jobs.parquet': jobs,\n",
    "        'job_skills.parquet': job_skills,\n",
    "        'skills.parquet': skills,\n",
    "        'job_industries.parquet': job_industries,\n",
    "        'industries.parquet': industries,\n",
    "    }\n",
    "    \n",
    "    total_size = 0\n",
    "    for filename, df in files.items():\n",
    "        filepath = PROCESSED_DIR / filename\n",
    "        df.to_parquet(filepath, index=False)\n",
    "        \n",
    "        # Get file size\n",
    "        size_mb = filepath.stat().st_size / 1024**2\n",
    "        total_size += size_mb\n",
    "        \n",
    "        print(f\"  ‚úì {filename:25s} {size_mb:6.1f} MB  ({len(df):,} rows)\")\n",
    "    \n",
    "    print(f\"\\n  Total: {total_size:.1f} MB\")\n",
    "    print(f\"\\n‚úì All files saved to: {PROCESSED_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test save v·ªõi sample data\n",
    "print(\"Testing save function v·ªõi sample data...\")\n",
    "save_normalized_data(jobs_sample, job_skills, skills, job_industries, industries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7dd6e",
   "metadata": {},
   "source": [
    "## Part 6: Run Full Pipeline\n",
    "\n",
    "### 6.1 Load Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821ea74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  LOADING FULL DATASET - This will take 2-3 minutes...\n",
      "\n",
      "\n",
      "============================================================\n",
      "LOADING JOBS (Normalized)\n",
      "============================================================\n",
      "Step 1: Reading postings.csv...\n",
      "  ‚úì Loaded 123,849 rows\n",
      "\n",
      "Step 2: Selecting columns...\n",
      "\n",
      "Step 3: Cleaning data...\n",
      "  ‚úì Dropped 7 rows with missing title/description\n",
      "  ‚úì Dropped 0 duplicate job_id rows\n",
      "\n",
      "Step 4: Parsing location...\n",
      "  ‚úì Parsed into city, state, country\n",
      "\n",
      "Step 5: Normalizing salary to yearly...\n",
      "  ‚úì 29,792 jobs have salary data (24.1%)\n",
      "\n",
      "Step 6: Converting data types...\n",
      "  ‚úì Converted dtypes\n",
      "\n",
      "‚úì Final shape: (123842, 20)\n",
      "‚úì Memory usage: 879.3 MB\n",
      "\n",
      "============================================================\n",
      "LOADING JOB_SKILLS (Many-to-Many)\n",
      "============================================================\n",
      "‚úì Loaded 213,768 job-skill relationships\n",
      "‚úì Dropped 0 rows with missing values\n",
      "‚úì Memory usage: 12.5 MB\n",
      "\n",
      "============================================================\n",
      "LOADING SKILLS (Lookup Table)\n",
      "============================================================\n",
      "‚úì Loaded 35 skills\n",
      "\n",
      "============================================================\n",
      "LOADING JOB_INDUSTRIES (Many-to-Many)\n",
      "============================================================\n",
      "‚úì Loaded 164,808 job-industry relationships\n",
      "‚úì Dropped 0 rows with missing values\n",
      "‚úì Memory usage: 2.8 MB\n",
      "\n",
      "============================================================\n",
      "LOADING INDUSTRIES (Lookup Table)\n",
      "============================================================\n",
      "‚úì Loaded 422 industries\n",
      "\n",
      "‚úì All data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This will load full dataset (~123k jobs)\n",
    "# Uncomment to run\n",
    "\n",
    "print(\"‚ö†Ô∏è  LOADING FULL DATASET - This will take 2-3 minutes...\")\n",
    "print(\"\")\n",
    "\n",
    "# Load all data\n",
    "jobs_full = load_jobs_normalized(sample=None)  # Full dataset\n",
    "job_skills_full = load_job_skills()\n",
    "skills_full = load_skills()\n",
    "job_industries_full = load_job_industries()\n",
    "industries_full = load_industries()\n",
    "\n",
    "print(\"\\n‚úì All data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f54a98",
   "metadata": {},
   "source": [
    "### 6.2 Verify Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d026dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "1. JOBS TABLE\n",
      "   ‚úì Rows: 123,842\n",
      "   ‚úì Unique job_id: True\n",
      "   ‚úì No missing title: True\n",
      "   ‚úì No missing description: True\n",
      "   ‚úì Salary coverage: 24.1%\n",
      "\n",
      "2. JOB_SKILLS TABLE\n",
      "   ‚úì Rows: 213,768\n",
      "   ‚úì Unique jobs: 126,807\n",
      "   ‚úì Unique skills: 35\n",
      "   ‚úì Avg skills per job: 1.69\n",
      "\n",
      "3. SKILLS LOOKUP\n",
      "   ‚úì Total skills: 35\n",
      "\n",
      "4. JOB_INDUSTRIES TABLE\n",
      "   ‚úì Rows: 164,808\n",
      "   ‚úì Unique jobs: 127,125\n",
      "   ‚úì Unique industries: 422\n",
      "   ‚úì Avg industries per job: 1.30\n",
      "\n",
      "5. INDUSTRIES LOOKUP\n",
      "   ‚úì Total industries: 422\n",
      "\n",
      "6. FOREIGN KEY INTEGRITY\n",
      "   ‚úì Invalid skills in job_skills: 0\n",
      "   ‚úì Invalid industries in job_industries: 0\n",
      "\n",
      "============================================================\n",
      "‚úì ALL CHECKS PASSED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify full dataset\n",
    "verify_data_quality(\n",
    "    jobs_full, \n",
    "    job_skills_full, \n",
    "    skills_full, \n",
    "    job_industries_full, \n",
    "    industries_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a96780",
   "metadata": {},
   "source": [
    "### 6.3 Save Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e51eea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING NORMALIZED DATA\n",
      "============================================================\n",
      "  ‚úì jobs.parquet               224.7 MB  (123,842 rows)\n",
      "  ‚úì job_skills.parquet           1.1 MB  (213,768 rows)\n",
      "  ‚úì skills.parquet               0.0 MB  (35 rows)\n",
      "  ‚úì job_industries.parquet       1.0 MB  (164,808 rows)\n",
      "  ‚úì industries.parquet           0.0 MB  (422 rows)\n",
      "\n",
      "  Total: 226.9 MB\n",
      "\n",
      "‚úì All files saved to: /home/sakana/Code/DS-RS/data/processed\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üéâ DAY 1 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "‚úÖ Normalized data pipeline ready\n",
      "‚úÖ NO aggregation - skills/industries kept separate\n",
      "‚úÖ All data saved to data/processed/\n",
      "\n",
      "Next: Day 2 - BM25 Search Implementation\n"
     ]
    }
   ],
   "source": [
    "# Save full dataset\n",
    "save_normalized_data(\n",
    "    jobs_full, \n",
    "    job_skills_full, \n",
    "    skills_full, \n",
    "    job_industries_full, \n",
    "    industries_full\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DAY 1 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Normalized data pipeline ready\")\n",
    "print(\"‚úÖ NO aggregation - skills/industries kept separate\")\n",
    "print(\"‚úÖ All data saved to data/processed/\")\n",
    "print(\"\\nNext: Day 2 - BM25 Search Implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f5a91",
   "metadata": {},
   "source": [
    "## Part 7: Final Summary & Storage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d67ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STORAGE COMPARISON\n",
      "============================================================\n",
      "\n",
      "‚ùå OLD (Aggregated):\n",
      "   clean_jobs.parquet: 674.4 MB\n",
      "\n",
      "‚úÖ NEW (Normalized):\n",
      "   jobs.parquet               224.7 MB\n",
      "   job_skills.parquet           1.1 MB\n",
      "   skills.parquet               0.0 MB\n",
      "   job_industries.parquet       1.0 MB\n",
      "   industries.parquet           0.0 MB\n",
      "\n",
      "   TOTAL: 226.9 MB\n",
      "\n",
      "üíæ SAVINGS: 447.5 MB (66.4%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calculate total size of all files in directory.\"\"\"\n",
    "    total = 0\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_file():\n",
    "            total += entry.stat().st_size\n",
    "        elif entry.is_dir():\n",
    "            total += get_directory_size(entry.path)\n",
    "    return total\n",
    "\n",
    "# Compare storage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STORAGE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Old (aggregated)\n",
    "old_file = PROCESSED_DIR / 'clean_jobs.parquet'\n",
    "if old_file.exists():\n",
    "    old_size = old_file.stat().st_size / 1024**2\n",
    "    print(f\"\\n‚ùå OLD (Aggregated):\")\n",
    "    print(f\"   clean_jobs.parquet: {old_size:.1f} MB\")\n",
    "\n",
    "# New (normalized)\n",
    "new_files = ['jobs.parquet', 'job_skills.parquet', 'skills.parquet', \n",
    "             'job_industries.parquet', 'industries.parquet']\n",
    "new_total = 0\n",
    "\n",
    "print(f\"\\n‚úÖ NEW (Normalized):\")\n",
    "for filename in new_files:\n",
    "    filepath = PROCESSED_DIR / filename\n",
    "    if filepath.exists():\n",
    "        size = filepath.stat().st_size / 1024**2\n",
    "        new_total += size\n",
    "        print(f\"   {filename:25s} {size:6.1f} MB\")\n",
    "\n",
    "print(f\"\\n   TOTAL: {new_total:.1f} MB\")\n",
    "\n",
    "if old_file.exists():\n",
    "    savings = (old_size - new_total) / old_size * 100\n",
    "    print(f\"\\nüíæ SAVINGS: {old_size - new_total:.1f} MB ({savings:.1f}%)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
