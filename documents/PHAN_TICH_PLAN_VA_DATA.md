# PHÃ‚N TÃCH PLAN VÃ€ DATA ÄÃƒ TIá»€N Xá»¬ LÃ

**NgÃ y phÃ¢n tÃ­ch**: 27/12/2025  
**Má»¥c Ä‘Ã­ch**: So sÃ¡nh plan redesign vá»›i data hiá»‡n táº¡i, xÃ¡c Ä‘á»‹nh gap vÃ  action items

---

## ğŸ“Š 1. Tá»”NG QUAN DATA HIá»†N Táº I

### 1.1 Raw Data Structure

**Dá»¯ liá»‡u gá»‘c** (trong `data/raw/`):
```
data/raw/
â”œâ”€â”€ postings.csv                    # Main table - 123,849 jobs
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ job_skills.csv             # Many-to-many: job â†’ skills
â”‚   â”œâ”€â”€ job_industries.csv         # Many-to-many: job â†’ industries
â”‚   â”œâ”€â”€ benefits.csv               # Job benefits
â”‚   â””â”€â”€ salaries.csv               # Salary info
â”œâ”€â”€ companies/
â”‚   â”œâ”€â”€ companies.csv              # Company metadata
â”‚   â”œâ”€â”€ company_specialities.csv   # Company tags
â”‚   â”œâ”€â”€ company_industries.csv     # Company industries
â”‚   â””â”€â”€ employee_counts.csv        # Employee count history
â””â”€â”€ mappings/
    â”œâ”€â”€ skills.csv                 # skill_abr â†’ skill_name (36 skills)
    â””â”€â”€ industries.csv             # industry_id â†’ industry_name (422 industries)
```

**Tá»•ng sá»‘ dÃ²ng**: ~3.8M rows (táº¥t cáº£ files combined)

**KÃ­ch thÆ°á»›c**:
- `data/processed/clean_jobs.parquet`: **675 MB** (aggregated)
- Raw CSVs: ChÆ°a xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c (cáº§n check)

### 1.2 Processed Data (Hiá»‡n táº¡i)

**File**: `data/processed/clean_jobs.parquet` (675 MB)

**CÃ¡ch xá»­ lÃ½ hiá»‡n táº¡i** (trong `src/loader.py`):
```python
# âŒ PROBLEM: Aggregation - máº¥t cáº¥u trÃºc quan há»‡
def build_enriched_jobs():
    # Skills: JOIN nhiá»u rows â†’ gá»™p thÃ nh 1 string
    skill_agg = (
        skills.groupby("job_id")["skill_name"]
        .apply(_collapse_unique)  # "Python, Java, SQL"
        .reset_index()
    )
    
    # Industries: JOIN nhiá»u rows â†’ gá»™p thÃ nh 1 string  
    industry_agg = (
        job_ind.groupby("job_id")["industry_name"]
        .apply(_collapse_unique)  # "IT, Healthcare, Finance"
        .reset_index()
    )
    
    # Benefits, Company specs: cÅ©ng bá»‹ gá»™p string
    # ...
    
    # Merge táº¥t cáº£ vÃ o 1 table lá»›n
    enriched = postings.merge(skill_agg, on="job_id")
                      .merge(industry_agg, on="job_id")
                      .merge(benefits_agg, on="job_id")
                      # ... 8 JOINs
```

**Káº¿t quáº£**: 
- âœ… CÃ³ 1 table duy nháº¥t â†’ dá»… query
- âŒ Skills/industries thÃ nh string â†’ **khÃ´ng filter chÃ­nh xÃ¡c Ä‘Æ°á»£c**
- âŒ Duplicate data (skills "Python" láº·p láº¡i trong nhiá»u jobs)
- âŒ 675 MB (lá»›n, tá»‘n memory)

---

## ğŸ¯ 2. YÃŠU Cáº¦U Tá»ª REDESIGN PLAN

### 2.1 Data Architecture Má»›i

**Plan yÃªu cáº§u** (tá»« REDESIGN_COMPLETE_PLAN.md):

```python
# âœ… SOLUTION: Normalized tables - GIá»® NGUYÃŠN quan há»‡
tables = {
    'jobs': {
        'size': '~50 MB',
        'rows': 123_842,
        'columns': ['job_id', 'title', 'description', 'company_id', 
                    'location', 'city', 'state', 'country',
                    'work_type', 'experience_level', 'remote_allowed',
                    'min_salary', 'max_salary', 'normalized_salary_yearly']
    },
    'job_skills': {
        'size': '~3 MB', 
        'rows': 213_768,  # Estimated
        'columns': ['job_id', 'skill_abr']
    },
    'skills': {
        'size': '<1 MB',
        'rows': 36,
        'columns': ['skill_abr', 'skill_name']
    },
    'job_industries': {
        'size': '~3 MB',
        'rows': 164_808,  # Estimated
        'columns': ['job_id', 'industry_id']
    },
    'industries': {
        'size': '<1 MB',
        'rows': 422,
        'columns': ['industry_id', 'industry_name']
    }
}
```

**Tá»•ng dung lÆ°á»£ng má»›i**: ~70 MB (giáº£m 90% tá»« 675 MB)

### 2.2 So sÃ¡nh Approach

| Aspect | Hiá»‡n táº¡i (âŒ) | Plan má»›i (âœ…) |
|--------|---------------|----------------|
| **Data Structure** | Aggregated (1 table) | Normalized (5+ tables) |
| **Skills storage** | `"Python, Java, SQL"` string | `job_skills` table (job_id, skill_abr) |
| **Filter logic** | Post-processing (sau search) | Pre-filtering (JOIN trÆ°á»›c search) |
| **Storage** | 675 MB | ~70 MB |
| **Filter accuracy** | KhÃ´ng chÃ­nh xÃ¡c (string matching) | 100% chÃ­nh xÃ¡c (relational JOIN) |
| **Search method** | TF-IDF + FAISS (vector) | BM25 (keyword) + optional embeddings |

---

## ğŸ” 3. Váº¤N Äá»€ Cá»¤ THá»‚ TRONG DATA HIá»†N Táº I

### 3.1 Problem 1: Skills Aggregation

**Code hiá»‡n táº¡i** (`src/loader.py:103`):
```python
def _collapse_unique(values: pd.Series) -> Optional[str]:
    """Convert a series of categorical values into a sorted, comma-separated string."""
    cleaned = {str(v).strip() for v in values if isinstance(v, str) and v.strip()}
    return ", ".join(sorted(cleaned)) if cleaned else None
```

**VÃ­ dá»¥ káº¿t quáº£**:
```
job_id=12345 â†’ skills = "Business Development, Information Technology, Python Programming"
```

**Táº¡i sao Ä‘Ã¢y lÃ  váº¥n Ä‘á»?**

1. **KhÃ´ng filter chÃ­nh xÃ¡c**:
   ```python
   # User filter: "Python"
   # Current: string contains "Python" â†’ MATCH
   # But also matches: "Python Script Writer", "Python Documentation"
   # â†’ SAI!
   ```

2. **KhÃ´ng thá»ƒ AND logic**:
   ```python
   # User muá»‘n: jobs cÃ³ Cáº¢ "Python" VÃ€ "SQL"
   # Current: check if "Python" in string AND "SQL" in string
   # Problem: "Python, SQL Analysis" â†’ SQL Analysis khÃ´ng pháº£i SQL skill
   ```

3. **Duplicate data**:
   ```
   Job 1: "Python, Java, SQL"
   Job 2: "Python, JavaScript, SQL"  
   Job 3: "Python, C++, SQL"
   
   â†’ "Python" vÃ  "SQL" Ä‘Æ°á»£c lÆ°u 3 láº§n (láº·p)
   â†’ Tá»‘n storage + memory
   ```

### 3.2 Problem 2: Location Parsing KhÃ´ng Äá»“ng nháº¥t

**Code hiá»‡n táº¡i** (`src/preprocessing.py:143`):
```python
def parse_location(location: str) -> dict:
    # Split by comma
    parts = [p.strip() for p in location.split(",")]
    
    if len(parts) == 2:
        # Two parts - city, state or city, country
        city, state_or_country = parts
        if len(state_or_country) == 2 and state_or_country.isupper():
            return {"city": city, "state": state_or_country, "country": "United States"}
        else:
            return {"city": city, "state": None, "country": state_or_country}
```

**Váº¥n Ä‘á»**:
- KhÃ´ng handle edge cases:
  - `"New York, NY, United States"` â†’ OK
  - `"New York"` â†’ country="New York" (SAI!)
  - `"Remote"` â†’ country="Remote" (SAI!)
  - `"United States"` â†’ handled riÃªng (inconsistent)

**Plan yÃªu cáº§u**:
```python
# Cáº§n parsing tá»‘t hÆ¡n vá»›i:
# 1. Handle "Remote" special case
# 2. City/State/Country standardization
# 3. Fallback values há»£p lÃ½
```

### 3.3 Problem 3: Salary Normalization

**Data audit** (`reports/data_audit.md`):
```
Salary coverage:
- min_salary: 75.9% thiáº¿u (chá»‰ 24% cÃ³)
- max_salary: 75.9% thiáº¿u
- med_salary: 94.9% thiáº¿u
- pay_period: 70.9% thiáº¿u

Pay period phÃ¢n bá»‘:
- YEARLY: 20,628 (16.7%)
- HOURLY: 14,741 (11.9%)
- MONTHLY: 518 (0.4%)
```

**Váº¥n Ä‘á»**:
1. **23% coverage ráº¥t tháº¥p** â†’ filter by salary = 0 results thÆ°á»ng xuyÃªn
2. **Conversion factors** cáº§n chuáº©n hÃ³a:
   ```python
   # Current (chÆ°a rÃµ rÃ ng):
   # HOURLY â†’ YEARLY: multiply by ???
   # Plan yÃªu cáº§u: 2080 hours/year (40h/week Ã— 52 weeks)
   ```

3. **Missing strategy**: Plan yÃªu cáº§u KHÃ”NG impute salary (giá»¯ NULL)
   - Hiá»‡n táº¡i: CÃ³ thá»ƒ Ä‘Ã£ impute (cáº§n check)

---

## âš ï¸ 4. GAPS GIá»®A PLAN VÃ€ IMPLEMENTATION HIá»†N Táº I

### 4.1 Data Pipeline Gaps

| Feature | Plan yÃªu cáº§u | Hiá»‡n táº¡i | Gap |
|---------|--------------|----------|-----|
| **Normalized tables** | âœ… Cáº§n | âŒ KhÃ´ng cÃ³ | **CRITICAL** |
| **Skills table** | RiÃªng biá»‡t (job_skills) | Aggregated string | **CRITICAL** |
| **Industries table** | RiÃªng biá»‡t (job_industries) | Aggregated string | **CRITICAL** |
| **Location parsing** | city, state, country fields | CÃ³ nhÆ°ng inconsistent | MEDIUM |
| **Salary normalization** | normalized_salary_yearly | CÃ³ partial | MEDIUM |
| **No aggregation** | Keep many-to-many | Aggregated everywhere | **CRITICAL** |

### 4.2 Search Architecture Gaps

| Component | Plan yÃªu cáº§u | Hiá»‡n táº¡i | Gap |
|-----------|--------------|----------|-----|
| **Search method** | BM25 (keyword) | TF-IDF + FAISS (vector) | **MAJOR** |
| **Filter timing** | Pre-filtering (trÆ°á»›c search) | Post-filtering (sau search) | **CRITICAL** |
| **Filter accuracy** | 100% (relational JOIN) | String matching (~60%) | **CRITICAL** |
| **Fallback** | NO fallback (honest 0 results) | 7-layer progressive fallback | MAJOR |
| **Field weights** | Title^3, Skills^2, Desc^1 | Equal weights (TF-IDF) | MEDIUM |

### 4.3 Code Gaps

| File | Plan yÃªu cáº§u | Hiá»‡n táº¡i | Status |
|------|--------------|----------|--------|
| `src/loader_v2.py` | Load normalized (no aggregate) | âŒ ChÆ°a cÃ³ | **TODO** |
| `src/bm25_search.py` | BM25 + pre-filters | âŒ ChÆ°a cÃ³ | **TODO** |
| `src/hybrid_search.py` | Hybrid (BM25 + semantic) | âŒ ChÆ°a cÃ³ | **TODO** |
| `src/evaluation.py` | Manual labeling framework | âŒ ChÆ°a cÃ³ | **TODO** |
| `app_v2.py` | UI má»›i (honest UX) | `app.py` (cÃ³ fallback) | **TODO** |
| `src/loader.py` | â†’ Deprecated | âœ… CÃ³ (353 lines) | **REPLACE** |
| `src/preprocessing.py` | â†’ Keep some utils | âœ… CÃ³ (368 lines) | **REFACTOR** |
| `src/recommender.py` | â†’ Deprecated | âœ… CÃ³ (623 lines) | **REPLACE** |

---

## ğŸ“‹ 5. ACTION ITEMS - Æ¯U TIÃŠN

### 5.1 CRITICAL (Day 1) - Data Pipeline

**Má»¥c tiÃªu**: Táº¡o normalized tables tá»« raw CSVs

**Tasks**:

1. **Táº¡o `src/loader_v2.py`** (2-3 giá»):
   ```python
   def load_jobs_normalized() -> pd.DataFrame:
       """Load jobs table - NO aggregation."""
       # Read postings.csv
       # Select columns cáº§n thiáº¿t
       # Clean: drop missing title/description
       # Remove duplicates by job_id
       # Parse location â†’ city, state, country
       # Normalize salary â†’ normalized_salary_yearly
       # Return: 123,842 rows Ã— 25 columns (~50 MB)
   
   def load_job_skills() -> pd.DataFrame:
       """Load job-skill relationships."""
       # Read jobs/job_skills.csv
       # Keep: job_id, skill_abr
       # Return: 213,768 rows Ã— 2 columns (~3 MB)
   
   def load_skills() -> pd.DataFrame:
       """Load skills lookup."""
       # Read mappings/skills.csv
       # Return: 36 rows
   
   def load_job_industries() -> pd.DataFrame:
       """Load job-industry relationships."""
       # Read jobs/job_industries.csv
       # Keep: job_id, industry_id
       # Return: 164,808 rows Ã— 2 columns (~3 MB)
   
   def load_industries() -> pd.DataFrame:
       """Load industries lookup."""
       # Read mappings/industries.csv
       # Return: 422 rows
   ```

2. **Save normalized tables** (30 phÃºt):
   ```python
   def save_normalized_data():
       # Save to data/processed/
       jobs.to_parquet('data/processed/jobs.parquet')
       job_skills.to_parquet('data/processed/job_skills.parquet')
       skills.to_parquet('data/processed/skills.parquet')
       job_industries.to_parquet('data/processed/job_industries.parquet')
       industries.to_parquet('data/processed/industries.parquet')
   ```

3. **Unit tests** (1 giá»):
   ```python
   # tests/test_loader_v2.py
   def test_load_jobs_normalized():
       assert jobs['job_id'].is_unique
       assert jobs['title'].notna().all()
       assert len(jobs) > 100000
   
   def test_load_job_skills():
       assert 'job_id' in job_skills.columns
       assert 'skill_abr' in job_skills.columns
   ```

**Kiá»ƒm tra thÃ nh cÃ´ng**:
- âœ… `data/processed/jobs.parquet` (~50 MB) tá»“n táº¡i
- âœ… `data/processed/job_skills.parquet` (~3 MB) tá»“n táº¡i
- âœ… `jobs['job_id'].is_unique == True`
- âœ… No duplicates
- âœ… No missing title/description
- âœ… Location parsed correctly (spot check)

### 5.2 HIGH (Day 2) - BM25 Search

**Má»¥c tiÃªu**: Implement BM25 search vá»›i pre-filtering

**Tasks**:

1. **Install dependency**:
   ```bash
   pip install rank-bm25
   ```

2. **Implement `src/bm25_search.py`** (3-4 giá»):
   ```python
   class BM25JobSearch:
       def __init__(self, jobs, job_skills, skills):
           # Build 3 separate BM25 indexes:
           # - Title corpus
           # - Description corpus  
           # - Skills corpus (JOIN job_skills â†’ skills)
       
       def search(self, query, top_k=1000):
           # Get scores from 3 indexes
           # Weighted combination: 3Ã—title + 2Ã—skills + 1Ã—desc
           # Return top-K
   
   def apply_filters(jobs, filters, job_skills=None):
       # Pre-filter BEFORE search:
       # 1. Location (fuzzy string match)
       # 2. Work type (exact)
       # 3. Experience level (exact)
       # 4. Remote (boolean)
       # 5. Salary range (numeric)
       # 6. Skills (JOIN job_skills)
       # Return: filtered DataFrame
   ```

3. **Test vÃ  benchmark** (1 giá»):
   ```python
   # Test queries
   queries = [
       "Python developer",
       "Data scientist machine learning",
       "Frontend engineer React"
   ]
   
   # Measure search time
   for q in queries:
       start = time.time()
       results = searcher.search(q, top_k=20)
       print(f"{q}: {(time.time()-start)*1000:.1f}ms")
   
   # Target: <100ms per query
   ```

**Kiá»ƒm tra thÃ nh cÃ´ng**:
- âœ… BM25 search tráº£ vá» káº¿t quáº£
- âœ… Search time <100ms
- âœ… Filters hoáº¡t Ä‘á»™ng (test tá»«ng loáº¡i)
- âœ… Skills filter chÃ­nh xÃ¡c (exact match)

### 5.3 MEDIUM (Day 3) - Evaluation

**Má»¥c tiÃªu**: Táº¡o test set vÃ  Ä‘o Precision@K

**Tasks**:

1. **Táº¡o test queries** (1 giá»):
   ```json
   // data/test_queries.json
   [
     {
       "query_id": 1,
       "query": "Python backend developer",
       "filters": {"work_type": "Full-time"},
       "relevant_job_ids": []  // Fill sau
     },
     // ... 19 queries ná»¯a
   ]
   ```

2. **Manual labeling** (2-3 giá»):
   - Cháº¡y má»—i query
   - Review top-10 results
   - Label: 2=perfect, 1=partial, 0=not relevant
   - Ghi job_ids relevant

3. **Implement metrics** (1 giá»):
   ```python
   # src/evaluation.py
   def precision_at_k(retrieved, relevant, k):
       top_k = retrieved[:k]
       return len(set(top_k) & set(relevant)) / k
   
   def evaluate_test_set(searcher, test_queries):
       # Run all queries
       # Calculate P@5, P@10, R@5, R@10
       # Return: DataFrame vá»›i results
   ```

**Target**: Precision@5 â‰¥ 80%

### 5.4 LOW (Day 4-5) - UI + Polish

**Tasks**:
1. Implement hybrid search (BM25 + semantic)
2. Create `app_v2.py` (Streamlit)
3. End-to-end testing
4. Documentation

---

## ğŸ”¢ 6. EXPECTED STORAGE COMPARISON

### 6.1 Current (Aggregated)

```
data/processed/
â””â”€â”€ clean_jobs.parquet    675 MB
    
Memory usage (loaded):
- Pandas DataFrame: ~1.2 GB RAM
- With TF-IDF vectors: +500 MB
- FAISS index: +200 MB
Total: ~1.9 GB RAM
```

### 6.2 Plan (Normalized)

```
data/processed/
â”œâ”€â”€ jobs.parquet           ~50 MB   (123,842 rows)
â”œâ”€â”€ job_skills.parquet      ~3 MB   (213,768 rows)
â”œâ”€â”€ skills.parquet          <1 MB   (36 rows)
â”œâ”€â”€ job_industries.parquet  ~3 MB   (164,808 rows)
â”œâ”€â”€ industries.parquet      <1 MB   (422 rows)
â””â”€â”€ companies.parquet      ~10 MB   (24,473 rows)
Total: ~70 MB

Memory usage (loaded):
- All tables: ~150 MB RAM
- BM25 indexes: ~100 MB
- Optional embeddings: ~200 MB (if hybrid)
Total: ~450 MB RAM (giáº£m 76%)
```

**Savings**:
- Storage: 675 MB â†’ 70 MB (**-90%**)
- RAM: 1.9 GB â†’ 450 MB (**-76%**)

---

## ğŸ¯ 7. Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š

### 7.1 TÃ³m táº¯t Váº¥n Ä‘á»

**3 váº¥n Ä‘á» nghiÃªm trá»ng** trong data hiá»‡n táº¡i:

1. **Data aggregation** â†’ Máº¥t cáº¥u trÃºc quan há»‡
   - Skills/industries thÃ nh string
   - KhÃ´ng filter chÃ­nh xÃ¡c
   - Tá»‘n storage (duplicate)

2. **Post-filtering** â†’ Search khÃ´ng hiá»‡u quáº£
   - Search táº¥t cáº£ 123k jobs
   - Rá»“i má»›i filter
   - Káº¿t quáº£ khÃ´ng Ä‘Ã¡ng tin

3. **Progressive fallback** â†’ UX misleading
   - 0 results â†’ relax filters
   - Tráº£ vá» jobs khÃ´ng liÃªn quan
   - User bá»‘i rá»‘i

### 7.2 Khuyáº¿n nghá»‹

**Æ¯u tiÃªn CAO nháº¥t**: Implement Day 1 (Data Pipeline)

**LÃ½ do**:
- Táº¥t cáº£ cÃ¡c component khÃ¡c phá»¥ thuá»™c vÃ o normalized data
- Chá»‰ 3-4 giá» work
- Test Ä‘Æ°á»£c ngay (unit tests)
- Unlock Day 2, 3, 4

**BÆ°á»›c tiáº¿p theo**:
1. âœ… Táº¡o `src/loader_v2.py` (NGAY BÃ‚Y GIá»œ)
2. âœ… Test vá»›i sample (1000 jobs)
3. âœ… Run full dataset
4. âœ… Save normalized tables
5. âœ… Verify storage (~70 MB)

**Timeline thá»±c táº¿**:
- Day 1 (hÃ´m nay): Data pipeline (3-4h)
- Day 2 (28/12): BM25 search (4-5h)
- Day 3 (29/12): Evaluation (4-5h)
- Day 4 (30/12): UI + polish (4-5h)
- Day 5 (31/12): Buffer + documentation

**Tá»•ng**: ~20 giá» work (4 days Ã— 5h/day)

---

## ğŸ“Œ 8. NEXT IMMEDIATE ACTIONS

**HÃ€NH Äá»˜NG NGAY** (trong 30 phÃºt):

```bash
# 1. Create file
touch src/loader_v2.py

# 2. Start implementing (copy tá»« plan):
# - load_jobs_normalized()
# - parse_location()
# - normalize_salary_to_yearly()

# 3. Test vá»›i sample
python -c "from src.loader_v2 import load_jobs_normalized; print(load_jobs_normalized().shape)"
```

**READY TO START?** ğŸš€

---

**Status**: ğŸ”´ DATA PIPELINE CHÆ¯A Sáº´N SÃ€NG  
**Blocker**: Cáº§n implement `loader_v2.py` trÆ°á»›c  
**ETA**: 3-4 giá» (Day 1)  
**Last Updated**: 27/12/2025
