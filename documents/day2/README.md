# Day 2: Data Cleaning - Quick Start Guide

## ‚úÖ What's Been Completed

All Day 2 tasks from `plan.md` are now complete:

1. ‚úÖ Text normalization (HTML removal, unicode, special chars)
2. ‚úÖ Location parsing (city/state/country extraction)
3. ‚úÖ Duplicate removal and data validation
4. ‚úÖ Feature engineering (content field, binary flags, normalized salary)
5. ‚úÖ Complete cleaning pipeline with quality checks

## üöÄ How to Run the Cleaning Pipeline

### Option 1: Quick Command Line (Recommended for First Run)

Test with a small sample first:

```bash
python run_cleaning.py --sample 5000
```

Then run on full dataset:

```bash
python run_cleaning.py
```

### Option 2: Jupyter Notebook (Recommended for Exploration)

```bash
jupyter lab notebooks/1_data_cleaning.ipynb
```

Execute all cells sequentially. The notebook includes:

- Function tests
- Sample processing (5K jobs)
- Full dataset processing (124K jobs)
- Quality validation
- Visualizations
- Auto-generated report

### Option 3: Python Script

```python
from src.loader import build_and_clean_jobs

# Quick test
sample = build_and_clean_jobs(sample=1000, persist=False)
print(sample[['title', 'content', 'skills']].head())

# Full processing
full = build_and_clean_jobs(sample=None, persist=True)
```

## üìä Expected Results

After running the pipeline, you'll have:

1. **`data/processed/clean_jobs.parquet`** (~40-60 MB)

   - ~110-115K cleaned jobs
   - Combined `content` field for vectorization
   - Parsed location (city/state/country)
   - Enriched with skills, industries, benefits, company data

2. **`reports/data_cleaning_report.md`** (auto-generated)

   - Comprehensive statistics
   - Quality metrics
   - Feature completeness analysis

3. **`images/data_cleaning_summary.png`** (if running notebook)
   - 4-panel visualization
   - Content length distribution
   - Work type breakdown
   - Feature completeness

## üîç Key Cleaned Columns

The output dataset has these important columns:

| Column                     | Description                                                                           |
| -------------------------- | ------------------------------------------------------------------------------------- |
| `content`                  | Combined cleaned text (title + description + skills) - **use this for vectorization** |
| `title_clean`              | Cleaned job title                                                                     |
| `description_clean`        | Cleaned job description                                                               |
| `skills`                   | Comma-separated skill names (e.g., "Python, SQL, Docker")                             |
| `industries`               | Comma-separated industry names                                                        |
| `city`, `state`, `country` | Parsed location components                                                            |
| `work_type`                | Full-time, Contract, Part-time, etc.                                                  |
| `has_salary_info`          | Binary flag: 1 if salary data exists                                                  |
| `is_remote`                | Binary flag: 1 if remote allowed                                                      |
| `normalized_salary`        | Salary converted to yearly equivalent                                                 |

## üìù Quality Checks

The pipeline performs these quality checks:

- ‚úÖ Removes jobs missing title or description
- ‚úÖ Deduplicates by job_id
- ‚úÖ Validates ~100% content completeness
- ‚úÖ Reports feature coverage (skills: ~65%, industries: ~70%)
- ‚úÖ Tracks data quality metrics throughout

## ‚ö° Performance Notes

- **Sample (5K jobs)**: ~30 seconds
- **Full dataset (124K jobs)**: 5-10 minutes
- **Memory usage**: ~500MB-1GB peak
- **Output size**: ~40-60 MB (parquet), ~200MB (csv)

If you have memory constraints, process in batches:

```python
for i in range(0, 124000, 10000):
    batch = build_and_clean_jobs(sample=10000, persist=True,
                                   output_name=f"batch_{i}.parquet")
```

## üêõ Troubleshooting

### Issue: Import errors

```bash
pip install pandas numpy scikit-learn pyarrow
```

### Issue: FileNotFoundError

Make sure you've run Day 1 setup:

```python
# In notebooks/1_data_cleaning.ipynb
from pathlib import Path
import shutil

archive_dir = Path("../data/archive")
raw_dir = Path("../data/raw")
sync_archive(archive_dir, raw_dir)
```

### Issue: Memory error

Use smaller sample:

```python
build_and_clean_jobs(sample=10000, persist=True)
```

## üìö Code Reference

### Text Cleaning Functions (`src/preprocessing.py`)

```python
from src.preprocessing import clean_text, parse_location, prepare_features

# Clean individual text
cleaned = clean_text("<p>Python Developer</p>")
# Output: "python developer"

# Parse location
location = parse_location("New York, NY")
# Output: {"city": "New York", "state": "NY", "country": "United States"}

# Clean full dataframe
df_cleaned = prepare_features(df_raw)
```

### Data Loading Functions (`src/loader.py`)

```python
from src.loader import build_enriched_jobs, build_and_clean_jobs, load_cleaned_jobs

# Build enriched (not cleaned)
enriched = build_enriched_jobs(sample=1000)

# Build and clean (recommended)
cleaned = build_and_clean_jobs(sample=1000)

# Load previously cleaned data
df = load_cleaned_jobs()
```

## üéØ Next Steps

With Day 2 complete, you're ready for Day 3:

1. **Exploratory Data Analysis**

   - Generate word clouds from skills
   - Plot salary distributions by role/location
   - Analyze job trends over time
   - Create correlation heatmaps

2. **Advanced Visualizations**

   - Top skills by industry
   - Geographic distribution maps
   - Work type vs salary analysis
   - Remote vs onsite trends

3. **Prepare for Model Building**
   - Understand data distribution
   - Identify feature importance
   - Plan vectorization strategy

## üìñ Documentation

- **Day 2 Summary**: `documents/day2/day2_completion_summary.md`
- **Data Audit Report**: `reports/data_audit.md` (Day 1)
- **Cleaning Report**: `reports/data_cleaning_report.md` (auto-generated)
- **Project Plan**: `documents/plan.md`

## ‚ú® Summary

Day 2 is **COMPLETE**! You now have:

‚úÖ Production-ready data cleaning pipeline  
‚úÖ Cleaned dataset with 110-115K jobs  
‚úÖ Combined `content` field for vectorization  
‚úÖ Parsed location and enriched metadata  
‚úÖ Quality validation and reporting

The cleaned data is ready for EDA and model building! üöÄ
