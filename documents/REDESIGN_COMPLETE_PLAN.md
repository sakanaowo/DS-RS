# Job Recommendation System - Complete Redesign Plan

**Date**: December 26, 2025  
**Status**: ğŸ”„ FRESH START - Complete Redesign  
**Approach**: Hybrid (BM25 70% + Embeddings 30%)  
**Timeline**: 4-5 days (Dec 26-30, 2025)

---

## ï¿½ TL;DR

**Váº¥n Ä‘á»**: Há»‡ thá»‘ng hiá»‡n táº¡i (2,479 dÃ²ng code) cÃ³ lá»—i thiáº¿t káº¿ tá»« Ä‘áº§u - data bá»‹ aggregate thÃ nh chuá»—i vÄƒn báº£n, filter khÃ´ng hoáº¡t Ä‘á»™ng Ä‘Ãºng, progressive fallback tráº£ vá» káº¿t quáº£ khÃ´ng liÃªn quan.

**Giáº£i phÃ¡p**: XÃ¢y dá»±ng láº¡i hoÃ n toÃ n vá»›i:
- **Data**: Giá»¯ normalized tables (jobs, job_skills, skills) - KHÃ”NG aggregate
- **Search**: Hybrid BM25 (70%) + Embeddings (30%)
- **Filter**: Pre-filtering (hard constraints) - Ã¡p dá»¥ng TRÆ¯á»šC khi search
- **UX**: Honest - cháº¥p nháº­n 0 káº¿t quáº£ náº¿u filter quÃ¡ strict

**Timeline**: 5 ngÃ y (26-30/12/2025)
- **Day 1**: Data pipeline má»›i (normalized, ~70MB vs 493MB cÅ©)
- **Day 2**: BM25 search + filters (<100ms)
- **Day 3**: Evaluation framework (manual labeling 20 queries)
- **Day 4**: Hybrid search + UI má»›i (Streamlit)
- **Day 5**: Documentation + buffer

**Má»¥c tiÃªu**: Precision@5 â‰¥ 80%, search time <100ms (BM25), filters hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c 100%

**LÃ½ do redesign**: 
1. âŒ Skills/industries bá»‹ gá»™p thÃ nh string "Python, Java, SQL" â†’ khÃ´ng filter Ä‘Æ°á»£c chÃ­nh xÃ¡c
2. âŒ Filter Ã¡p dá»¥ng SAU khi search â†’ cháº­m, khÃ´ng Ä‘Ã¡ng tin cáº­y
3. âŒ Progressive fallback (7 layers) â†’ tráº£ vá» job "Nurse á»Ÿ Texas" cho query "Python developer á»Ÿ California, $100k+"
4. âœ… Solution: Normalized data + BM25 baseline + Pre-filtering + Honest UX

**Code cáº§n viáº¿t**: ~1,500 dÃ²ng má»›i (vs 2,479 dÃ²ng cÅ© sáº½ bá»)
- `src/loader_v2.py` (~300 lines) - Load normalized data
- `src/bm25_search.py` (~400 lines) - BM25 + filters
- `src/hybrid_search.py` (~300 lines) - Hybrid scoring
- `src/evaluation.py` (~150 lines) - Metrics
- `app_v2.py` (~200 lines) - UI má»›i
- `tests/` (~150 lines) - Unit tests

**Quyáº¿t Ä‘á»‹nh Ä‘Ã£ confirm** (Option 1A + 2B + 3A + 4A + 5A):
- âœ… Normalized schema (tá»‘t nháº¥t vá» cháº¥t lÆ°á»£ng)
- âœ… Hybrid search (BM25 70% + Semantic 30%)
- âœ… Pre-filtering (filters as query clauses)
- âœ… Manual evaluation (20 queries Ã— 10 jobs = 200 labels)
- âœ… 4-5 days timeline (Ä‘áº§y Ä‘á»§, khÃ´ng vá»™i vÃ ng)

---

## ï¿½ğŸ“‹ EXECUTIVE SUMMARY

### Why Redesign?

**Critical Flaws in Current Implementation**:
1. âŒ Skills/industries aggregated into comma-separated strings â†’ Lost relational structure
2. âŒ Filters applied AFTER search (post-processing) â†’ Inefficient, unreliable
3. âŒ Progressive fallback returns irrelevant results â†’ Poor UX
4. âŒ Cannot filter by individual skills â†’ "Python, Java, SQL" matches as one string

**Example of Current Problem**:
```
User: "Python developer in California, Full-time, $100k-150k"
Current System:
  â†’ Search 50k jobs with embeddings
  â†’ Apply filters (post-processing)
  â†’ Get 0 results (salary data 23% coverage)
  â†’ Progressive fallback (7 layers)
  â†’ Return "Nurse in Texas, Part-time, $50k" âŒ COMPLETELY IRRELEVANT
```

### New Design Solution

**Approach**: Normalized data + BM25 baseline + Optional embeddings + Pre-filtering

```
User Query â†’ Parse â†’ Pre-filter â†’ BM25 Search â†’ Optional Semantic Boost â†’ Rank â†’ Results
```

**Thay Ä‘á»•i chÃ­nh**:
1. âœ… Giá»¯ normalized tables (jobs, job_skills, skills) - KHÃ”NG aggregate
2. âœ… BM25 vá»›i field weights (Title^3, Skills^2, Description^1)
3. âœ… Filters lÃ  hard constraints - Ã¡p dá»¥ng TRÆ¯á»šC khi search
4. âœ… Hybrid scoring: 0.7 Ã— BM25 + 0.3 Ã— Semantic
5. âœ… Honest UX: Cháº¥p nháº­n 0 káº¿t quáº£ náº¿u filters quÃ¡ strict

---

## ğŸ¯ YÃŠU Cáº¦U Dá»° ÃN (tá»« FinalProject_recommendation_system.md)

### Must Have (CÆ¡ báº£n)
- âœ… Dataset â‰¥ 2,000 items â†’ Have 123,842 jobs
- âœ… â‰¥ 5 features â†’ Have 10+ (title, description, skills, industry, location, salary, work_type, experience, remote, company)
- âœ… Recommendation system â†’ BM25 + embeddings
- âœ… UI â†’ Streamlit
- âœ… 3 data processing tasks â†’ Missing values, duplicates, vectorization

### Nice to Have (NÃ¢ng cao - Ä‘iá»ƒm thÆ°á»Ÿng)
- âœ… Advanced embeddings â†’ MiniLM-L6-v2
- âœ… Context-aware â†’ Pre-filtering dá»±a trÃªn user filters
- âœ… User history â†’ Query logging (Ä‘Ã£ cÃ³)
- âš ï¸ Deploy cloud â†’ Optional náº¿u cÃ²n thá»i gian

### YÃªu cáº§u Evaluation
- âœ… Precision@K, Recall@K â†’ Sáº½ implement vá»›i manual labeling
- âœ… User study â†’ 10-20 test queries vá»›i manual relevance judgments

---

## ğŸ“Š KIáº¾N TRÃšC Dá»® LIá»†U Má»šI

### NguyÃªn táº¯c Thiáº¿t káº¿: **Giá»¯ Normalized, JOIN khi cáº§n**

### Báº£ng ChÃ­nh

#### 1. `jobs` (Báº£ng Dá»¯ liá»‡u ChÃ­nh)
```python
jobs = pd.DataFrame({
    # Primary key
    'job_id': int,  # Unique identifier
    
    # Content fields (for search)
    'title': str,  # "Senior Python Developer"
    'description': str,  # Long text (500-2000 chars)
    
    # Company info
    'company_id': int,
    'company_name': str,  # "Google"
    
    # Location (parsed)
    'location': str,  # Original "San Francisco, CA, United States"
    'city': str,  # "San Francisco"
    'state': str,  # "CA"
    'country': str,  # "United States"
    
    # Job metadata (for filtering)
    'work_type': str,  # "Full-time" | "Part-time" | "Contract" | "Internship" | "Temporary"
    'experience_level': str,  # "Entry level" | "Mid-Senior level" | "Director" | "Executive"
    'remote_allowed': bool,  # True/False (nullable for unknown)
    
    # Salary (sparse: ~23% coverage)
    'min_salary': float,  # Nullable
    'max_salary': float,  # Nullable
    'pay_period': str,  # "YEARLY" | "HOURLY" | "MONTHLY"
    'normalized_salary_yearly': float,  # Calculated: convert to yearly
    
    # Engagement metrics
    'views': int,
    'applies': int,
    'listed_time': datetime,
    'closed_time': datetime,  # Nullable
})

# Size: 123,842 rows Ã— ~25 columns = ~50 MB
```

#### 2. `job_skills` (Quan há»‡ Many-to-Many)
```python
job_skills = pd.DataFrame({
    'job_id': int,  # Foreign key to jobs
    'skill_abr': str,  # "IT", "PYTHON", "SQL", etc.
})

# Size: 213,768 rows Ã— 2 columns = ~3 MB
# Example:
#   job_id  skill_abr
#   12345   IT
#   12345   PYTHON
#   12345   SQL
#   23456   SALE
```

**Táº¡i sao tÃ¡ch báº£ng riÃªng?**
- âœ… Filter chÃ­nh xÃ¡c: "jobs cÃ³ Python skill" â†’ JOIN WHERE skill_abr='PYTHON'
- âœ… KhÃ´ng parse string: TrÃ¡nh "Python" match vá»›i "Python Script Writer"
- âœ… Hiá»‡u quáº£: Chá»‰ lÆ°u relationship, khÃ´ng láº·p láº¡i tÃªn skill

#### 3. `skills` (Báº£ng Tra cá»©u)
```python
skills = pd.DataFrame({
    'skill_abr': str,  # "IT", "PYTHON" (Primary key)
    'skill_name': str,  # "Information Technology", "Python Programming"
})

# Size: 36 rows (small lookup)
```

#### 4. `job_industries` (Quan há»‡ Many-to-Many)
```python
job_industries = pd.DataFrame({
    'job_id': int,
    'industry_id': int,
})

# Size: 164,808 rows Ã— 2 columns = ~3 MB
```

#### 5. `industries` (Báº£ng Tra cá»©u)
```python
industries = pd.DataFrame({
    'industry_id': int,
    'industry_name': str,  # "Hospitals and Health Care", "IT Services"
})

# Size: 422 rows
```

#### 6. `companies` (TÃ¹y chá»n - Ä‘á»ƒ hiá»ƒn thá»‹)
```python
companies = pd.DataFrame({
    'company_id': int,
    'company_name': str,
    'description': str,
    'company_size': int,
    'employee_count': int,
    'city': str,
    'state': str,
    'country': str,
})

# Size: 24,473 rows Ã— ~15 columns = ~10 MB
```

### Tá»•ng dung lÆ°á»£ng
- Hiá»‡n táº¡i: `postings.csv` = 493 MB (cÃ³ dá»¯ liá»‡u duplicate/aggregate)
- Má»›i: Táº¥t cáº£ báº£ng = ~70 MB (normalized, khÃ´ng duplicate)
- **Tiáº¿t kiá»‡m: Giáº£m 85% dung lÆ°á»£ng**

---

## ğŸ” KIáº¾N TRÃšC TÃŒM KIáº¾M - Hybrid BM25 + Embeddings

### Component 1: BM25 Index (ChÃ­nh - 70% trá»ng sá»‘)

**ThÆ° viá»‡n**: `rank-bm25` (pure Python, Ä‘Æ¡n giáº£n, Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh)

#### Táº¡i sao chá»n BM25?
- âœ… **Industry standard** cho search (Elasticsearch, Lucene dÃ¹ng BM25)
- âœ… **ÄÃ£ Ä‘Æ°á»£c chá»©ng minh**: Hoáº¡t Ä‘á»™ng tá»‘t cho keyword matching
- âœ… **Nhanh**: <100ms cho 100k documents
- âœ… **Dá»… debug**: CÃ³ thá»ƒ tháº¥y terms nÃ o matched
- âœ… **KhÃ´ng cáº§n training**: Hoáº¡t Ä‘á»™ng ngay out-of-the-box

#### Implementation

```python
from rank_bm25 import BM25Okapi
import numpy as np

class BM25JobSearch:
    def __init__(self, jobs, job_skills, skills):
        """Build separate BM25 indexes for each field."""
        
        # 1. Build corpus for each field
        corpus_title = jobs['title'].fillna('').tolist()
        corpus_description = jobs['description'].fillna('').tolist()
        corpus_skills = self._get_skills_corpus(jobs['job_id'], job_skills, skills)
        
        # 2. Tokenize (simple whitespace + lowercase)
        self.tokens_title = [self._tokenize(text) for text in corpus_title]
        self.tokens_desc = [self._tokenize(text) for text in corpus_description]
        self.tokens_skills = [self._tokenize(text) for text in corpus_skills]
        
        # 3. Build BM25 indexes
        print("Building BM25 indexes...")
        self.bm25_title = BM25Okapi(self.tokens_title)
        self.bm25_description = BM25Okapi(self.tokens_desc)
        self.bm25_skills = BM25Okapi(self.tokens_skills)
        print("âœ“ BM25 indexes ready")
        
        self.jobs = jobs
    
    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization."""
        return text.lower().split()
    
    def _get_skills_corpus(self, job_ids, job_skills, skills):
        """Join job_skills with skills to get skill names for each job."""
        # Merge to get skill names
        js_enriched = job_skills.merge(skills, on='skill_abr', how='left')
        
        # Aggregate by job_id
        skills_by_job = (
            js_enriched
            .groupby('job_id')['skill_name']
            .apply(lambda x: ' '.join(x.dropna()))
            .reindex(job_ids, fill_value='')
        )
        return skills_by_job.tolist()
    
    def search(self, query: str, top_k: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
        """
        BM25 search with field weights.
        
        Returns:
            scores: Array of BM25 scores for each job
            indices: Array of job indices (sorted by score, descending)
        """
        query_tokens = self._tokenize(query)
        
        # Get scores from each index
        scores_title = self.bm25_title.get_scores(query_tokens)
        scores_desc = self.bm25_description.get_scores(query_tokens)
        scores_skills = self.bm25_skills.get_scores(query_tokens)
        
        # Weighted combination
        # Title is most important (concise, descriptive)
        # Skills are very important (precise matching)
        # Description is less important (noisy, long)
        final_scores = (
            3.0 * scores_title +      # Title weight: 3x
            2.0 * scores_skills +     # Skills weight: 2x
            1.0 * scores_desc         # Description weight: 1x
        )
        
        # Get top-K indices
        top_indices = np.argsort(final_scores)[-top_k:][::-1]
        
        return final_scores[top_indices], top_indices
```

**LÃ½ do chá»n Field Weight**:
- **Title (3.0)**: Quan trá»ng nháº¥t - ngáº¯n gá»n, mÃ´ táº£ rÃµ, user Ä‘á»c Ä‘áº§u tiÃªn
- **Skills (2.0)**: Ráº¥t quan trá»ng - matching chÃ­nh xÃ¡c (Python != Python Script Writer)
- **Description (1.0)**: Ãt quan trá»ng nháº¥t - dÃ i, nhiá»…u, thÆ°á»ng cÃ³ boilerplate

---

### Component 2: Filters (Hard Constraints)

**NguyÃªn táº¯c chÃ­nh**: Ãp dá»¥ng filters TRÆ¯á»šC hoáº·c TRONG search, KHÃ”NG pháº£i sau

#### Implementation

```python
def apply_filters(jobs_df, filters, job_skills=None):
    """
    Apply filters as boolean masks (hard constraints).
    
    Args:
        jobs_df: Jobs DataFrame
        filters: Dict with filter conditions
        job_skills: job_skills table (needed for skills filter)
    
    Returns:
        Filtered DataFrame
    """
    filtered = jobs_df.copy()
    
    # 1. Location filter (fuzzy string matching)
    if 'location' in filters:
        loc = filters['location'].lower()
        mask = (
            filtered['city'].str.lower().str.contains(loc, na=False) |
            filtered['state'].str.lower().str.contains(loc, na=False) |
            filtered['country'].str.lower().str.contains(loc, na=False) |
            filtered['location'].str.lower().str.contains(loc, na=False)
        )
        filtered = filtered[mask]
        print(f"  After location filter: {len(filtered)} jobs")
    
    # 2. Work type filter (exact match)
    if 'work_type' in filters:
        work_types = filters['work_type']
        if isinstance(work_types, str):
            work_types = [work_types]
        filtered = filtered[filtered['work_type'].isin(work_types)]
        print(f"  After work_type filter: {len(filtered)} jobs")
    
    # 3. Experience level filter (exact match)
    if 'experience_level' in filters:
        exp_level = filters['experience_level']
        filtered = filtered[filtered['experience_level'] == exp_level]
        print(f"  After experience filter: {len(filtered)} jobs")
    
    # 4. Remote filter (boolean)
    if 'remote_allowed' in filters:
        remote = filters['remote_allowed']
        if remote:
            # Only jobs explicitly marked as remote
            filtered = filtered[filtered['remote_allowed'] == True]
        else:
            # Only jobs explicitly marked as non-remote
            filtered = filtered[filtered['remote_allowed'] == False]
        print(f"  After remote filter: {len(filtered)} jobs")
    
    # 5. Salary range filter (only jobs WITH salary info)
    if 'min_salary' in filters or 'max_salary' in filters:
        # First: Only keep jobs that HAVE salary data
        has_salary = filtered['normalized_salary_yearly'].notna()
        filtered = filtered[has_salary]
        
        # Then: Apply range
        if 'min_salary' in filters:
            filtered = filtered[filtered['normalized_salary_yearly'] >= filters['min_salary']]
        if 'max_salary' in filters:
            filtered = filtered[filtered['normalized_salary_yearly'] <= filters['max_salary']]
        print(f"  After salary filter: {len(filtered)} jobs")
    
    # 6. Skills filter (requires JOIN with job_skills)
    if 'skills' in filters and job_skills is not None:
        required_skills = filters['skills']
        if isinstance(required_skills, str):
            required_skills = [required_skills]
        
        # Get jobs that have ALL required skills
        # Method: Count how many required skills each job has
        matching_jobs = (
            job_skills[job_skills['skill_abr'].isin(required_skills)]
            .groupby('job_id')
            .size()
        )
        
        # Only jobs with ALL skills
        jobs_with_all_skills = matching_jobs[matching_jobs == len(required_skills)].index
        filtered = filtered[filtered['job_id'].isin(jobs_with_all_skills)]
        print(f"  After skills filter: {len(filtered)} jobs")
    
    # 7. Industry filter (requires JOIN with job_industries)
    if 'industries' in filters and job_industries is not None:
        required_industries = filters['industries']
        if isinstance(required_industries, str):
            required_industries = [required_industries]
        
        jobs_in_industries = (
            job_industries[job_industries['industry_id'].isin(required_industries)]
            ['job_id'].unique()
        )
        filtered = filtered[filtered['job_id'].isin(jobs_in_industries)]
        print(f"  After industry filter: {len(filtered)} jobs")
    
    return filtered
```

**Náº¿u cÃ³ 0 káº¿t quáº£?**
- âœ… **HONEST**: Tráº£ vá» empty DataFrame
- âœ… **UI Message**: "KhÃ´ng cÃ³ jobs phÃ¹ há»£p. Thá»­ bá» bá»›t filters."
- âŒ **KHÃ”NG FALLBACK**: KhÃ´ng tráº£ vá» jobs khÃ´ng liÃªn quan

---

### Component 3: Semantic Layer (TÃ¹y chá»n - 30% trá»ng sá»‘)

**Model**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)

#### Khi nÃ o dÃ¹ng Semantic Search?
- âœ… User query **mÃ´ táº£** ("looking for a job where I can work with data")
- âœ… Synonym matching ("ML Engineer" nÃªn match "Machine Learning")
- âœ… TÄƒng recall (tÃ¬m jobs mÃ  BM25 bá» sÃ³t)

#### Implementation

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class SemanticSearch:
    def __init__(self, jobs):
        """Pre-compute embeddings for all jobs."""
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.jobs = jobs
        
        # Build content corpus (title + description)
        corpus = (jobs['title'] + ' ' + jobs['description']).fillna('').tolist()
        
        # Encode (this takes ~2-3 minutes for 100k jobs)
        print("Encoding jobs with MiniLM...")
        self.embeddings = self.model.encode(corpus, show_progress_bar=True, batch_size=32)
        print(f"âœ“ Encoded {len(corpus)} jobs")
    
    def search(self, query: str, top_k: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
        """
        Semantic search using cosine similarity.
        
        Returns:
            scores: Cosine similarity scores (0-1)
            indices: Job indices sorted by score
        """
        query_embedding = self.model.encode([query])
        scores = cosine_similarity(query_embedding, self.embeddings)[0]
        
        top_indices = np.argsort(scores)[-top_k:][::-1]
        return scores[top_indices], top_indices
```

---

### Component 4: Hybrid Search (Káº¿t há»£p BM25 + Semantic)

```python
class HybridJobSearch:
    def __init__(self, jobs, job_skills, skills, use_semantic=True):
        """Initialize both BM25 and semantic search."""
        self.jobs = jobs
        self.job_skills = job_skills
        self.skills = skills
        
        # BM25 (always)
        self.bm25 = BM25JobSearch(jobs, job_skills, skills)
        
        # Semantic (optional)
        self.semantic = None
        if use_semantic:
            self.semantic = SemanticSearch(jobs)
    
    def search(
        self, 
        query: str, 
        filters: Dict[str, Any] = None,
        top_k: int = 20,
        alpha: float = 0.7
    ) -> pd.DataFrame:
        """
        Hybrid search: BM25 (70%) + Semantic (30%)
        
        Args:
            query: User search query
            filters: Filter conditions (applied FIRST)
            top_k: Number of results to return
            alpha: BM25 weight (1-alpha = semantic weight)
        
        Returns:
            DataFrame with ranked results
        """
        print(f"\n{'='*80}")
        print(f"HYBRID SEARCH")
        print(f"{'='*80}")
        print(f"Query: '{query}'")
        print(f"Filters: {filters}")
        print(f"Alpha: {alpha} (BM25={alpha}, Semantic={1-alpha})")
        
        # Step 1: Apply filters FIRST
        if filters:
            filtered_jobs = apply_filters(self.jobs, filters, self.job_skills)
            print(f"\nâ†’ After filtering: {len(filtered_jobs)} jobs remain")
            
            if len(filtered_jobs) == 0:
                print("â†’ No jobs match filters. Returning empty results.")
                return pd.DataFrame()
        else:
            filtered_jobs = self.jobs
            print(f"\nâ†’ No filters applied: searching all {len(filtered_jobs)} jobs")
        
        # Step 2: BM25 search on filtered subset
        print(f"\nâ†’ Running BM25 search...")
        bm25_scores, bm25_indices = self.bm25.search(query, top_k=min(1000, len(filtered_jobs)))
        
        # Map indices back to filtered_jobs
        bm25_jobs = filtered_jobs.iloc[bm25_indices].copy()
        bm25_jobs['bm25_score'] = bm25_scores
        
        # Step 3: Semantic search on filtered subset (if enabled)
        if self.semantic is not None:
            print(f"â†’ Running semantic search...")
            sem_scores, sem_indices = self.semantic.search(query, top_k=min(1000, len(filtered_jobs)))
            
            sem_jobs = filtered_jobs.iloc[sem_indices].copy()
            sem_jobs['semantic_score'] = sem_scores
            
            # Merge BM25 and Semantic scores
            # Use job_id as key
            merged = bm25_jobs.merge(
                sem_jobs[['job_id', 'semantic_score']], 
                on='job_id', 
                how='left'
            )
            merged['semantic_score'] = merged['semantic_score'].fillna(0)
            
            # Normalize scores to [0, 1]
            if merged['bm25_score'].max() > 0:
                merged['bm25_norm'] = (
                    merged['bm25_score'] / merged['bm25_score'].max()
                )
            else:
                merged['bm25_norm'] = 0
            
            if merged['semantic_score'].max() > 0:
                merged['semantic_norm'] = merged['semantic_score']  # Already 0-1
            else:
                merged['semantic_norm'] = 0
            
            # Hybrid score
            merged['final_score'] = (
                alpha * merged['bm25_norm'] + 
                (1 - alpha) * merged['semantic_norm']
            )
        else:
            # Pure BM25
            merged = bm25_jobs.copy()
            merged['final_score'] = merged['bm25_score']
        
        # Step 4: Rank and return top-K
        results = merged.nlargest(top_k, 'final_score').copy()
        results['rank'] = range(1, len(results) + 1)
        
        print(f"\nâ†’ Returning top {len(results)} results")
        print(f"{'='*80}\n")
        
        return results
```

---

## ğŸ› ï¸ Káº¾ HOáº CH TRIá»‚N KHAI - 5 NGÃ€Y

### Day 1 (Dec 26): Refactor Data Pipeline

#### Má»¥c tiÃªu
- âœ… Táº¡o normalized data loading (KHÃ”NG aggregate)
- âœ… Clean data (xÃ³a missing title/description, deduplicate)
- âœ… Parse location (city, state, country)
- âœ… Normalize salary vá» yearly
- âœ… Unit tests

#### Files cáº§n táº¡o
- `src/loader_v2.py` (má»›i, thay tháº¿ `src/loader.py`)
- `tests/test_loader_v2.py`

#### Tasks chi tiáº¿t

**Task 1.1: Táº¡o `load_jobs_normalized()` (2 giá»)**
```python
def load_jobs_normalized() -> pd.DataFrame:
    """
    Load jobs table without aggregation.
    
    Columns:
        - job_id, title, description, company_id, company_name
        - location, work_type, experience_level, remote_allowed
        - min_salary, max_salary, pay_period, normalized_salary_yearly
        - views, applies, listed_time, closed_time
    """
    # 1. Load raw
    postings = pd.read_csv('data/raw/postings.csv')
    
    # 2. Select columns
    jobs = postings[[
        'job_id', 'title', 'description', 'company_id', 'company_name',
        'location', 'formatted_work_type', 'formatted_experience_level',
        'remote_allowed', 'min_salary', 'max_salary', 'pay_period',
        'views', 'applies', 'original_listed_time', 'closed_time'
    ]].copy()
    
    # 3. Clean
    # Drop jobs without title or description
    jobs = jobs[jobs['title'].notna() & jobs['description'].notna()]
    jobs = jobs[jobs['title'].str.strip() != '']
    jobs = jobs[jobs['description'].str.strip() != '']
    
    # Remove duplicates by job_id
    jobs = jobs.drop_duplicates(subset=['job_id'], keep='first')
    
    # 4. Parse location
    location_parsed = jobs['location'].fillna('Unknown').apply(parse_location)
    jobs['city'] = location_parsed.apply(lambda x: x['city'])
    jobs['state'] = location_parsed.apply(lambda x: x['state'])
    jobs['country'] = location_parsed.apply(lambda x: x['country'])
    
    # 5. Normalize salary
    jobs['normalized_salary_yearly'] = jobs.apply(normalize_salary_to_yearly, axis=1)
    
    # 6. Rename columns
    jobs = jobs.rename(columns={
        'formatted_work_type': 'work_type',
        'formatted_experience_level': 'experience_level',
        'original_listed_time': 'listed_time'
    })
    
    # 7. Convert dtypes
    jobs['job_id'] = jobs['job_id'].astype('Int64')
    jobs['company_id'] = jobs['company_id'].astype('Int64')
    jobs['remote_allowed'] = jobs['remote_allowed'].astype('boolean')
    jobs['listed_time'] = pd.to_datetime(jobs['listed_time'])
    jobs['closed_time'] = pd.to_datetime(jobs['closed_time'])
    
    return jobs

def parse_location(loc_str: str) -> Dict[str, str]:
    """Parse location string into city, state, country."""
    if pd.isna(loc_str) or loc_str.strip() == '':
        return {'city': 'Unknown', 'state': '', 'country': ''}
    
    parts = [p.strip() for p in loc_str.split(',')]
    
    if len(parts) >= 3:
        return {'city': parts[0], 'state': parts[1], 'country': parts[2]}
    elif len(parts) == 2:
        return {'city': parts[0], 'state': parts[1], 'country': ''}
    else:
        return {'city': parts[0], 'state': '', 'country': ''}

def normalize_salary_to_yearly(row: pd.Series) -> float:
    """Convert salary to yearly amount."""
    if pd.isna(row['min_salary']) or pd.isna(row['max_salary']):
        return None
    
    # Use median
    median = (row['min_salary'] + row['max_salary']) / 2
    
    # Convert based on pay_period
    period = str(row.get('pay_period', '')).upper()
    multipliers = {
        'YEARLY': 1,
        'MONTHLY': 12,
        'BIWEEKLY': 26,
        'WEEKLY': 52,
        'HOURLY': 2080,  # 40 hours/week Ã— 52 weeks
    }
    
    return median * multipliers.get(period, 1)
```

**Task 1.2: Táº¡o `load_job_skills()` vÃ  `load_skills()` (30 phÃºt)**
```python
def load_job_skills() -> pd.DataFrame:
    """Load job-skill relationships (NO aggregation)."""
    job_skills = pd.read_csv('data/raw/jobs/job_skills.csv')
    job_skills = job_skills[['job_id', 'skill_abr']].copy()
    job_skills['job_id'] = job_skills['job_id'].astype('Int64')
    return job_skills

def load_skills() -> pd.DataFrame:
    """Load skills lookup table."""
    skills = pd.read_csv('data/raw/mappings/skills.csv')
    return skills[['skill_abr', 'skill_name']].copy()
```

**Task 1.3: Viáº¿t unit tests (1 giá»)**
```python
# tests/test_loader_v2.py

def test_load_jobs_normalized():
    jobs = load_jobs_normalized()
    
    # Check size
    assert len(jobs) > 100000, "Should have >100k jobs"
    
    # Check no duplicates
    assert jobs['job_id'].is_unique, "job_id should be unique"
    
    # Check required columns
    required = ['job_id', 'title', 'description', 'city', 'state', 'country']
    assert all(col in jobs.columns for col in required)
    
    # Check no missing title/description
    assert jobs['title'].notna().all()
    assert jobs['description'].notna().all()
    
def test_load_job_skills():
    job_skills = load_job_skills()
    assert len(job_skills) > 200000
    assert 'job_id' in job_skills.columns
    assert 'skill_abr' in job_skills.columns
```

**Task 1.4: LÆ°u vÃ o processed/ (30 phÃºt)**
```python
def save_normalized_data():
    """Save all normalized tables."""
    jobs = load_jobs_normalized()
    job_skills = load_job_skills()
    skills = load_skills()
    
    jobs.to_parquet('data/processed/jobs.parquet', index=False)
    job_skills.to_parquet('data/processed/job_skills.parquet', index=False)
    skills.to_parquet('data/processed/skills.parquet', index=False)
    
    print(f"âœ“ Saved {len(jobs)} jobs")
    print(f"âœ“ Saved {len(job_skills)} job-skill relationships")
    print(f"âœ“ Saved {len(skills)} skills")
```

**Deliverables Day 1**:
- âœ… `src/loader_v2.py` (~300 dÃ²ng)
- âœ… Normalized tables trong `data/processed/`
- âœ… Unit tests pass

---

### Day 2 (Dec 27): Implement BM25 Search

#### Má»¥c tiÃªu
- âœ… CÃ i `rank-bm25`
- âœ… Implement BM25JobSearch class
- âœ… Implement filter logic
- âœ… Test vá»›i sample queries
- âœ… Benchmark performance

#### Files cáº§n táº¡o
- `src/bm25_search.py` (má»›i)
- `tests/test_bm25_search.py`

#### Tasks chi tiáº¿t

**Task 2.1: Setup (15 phÃºt)**
```bash
pip install rank-bm25
```

**Task 2.2: Implement BM25JobSearch (3 giá»)**
Xem section "Component 1: BM25 Index" á»Ÿ trÃªn Ä‘á»ƒ cÃ³ full code.

**Task 2.3: Implement apply_filters (1 giá»)**
Xem section "Component 2: Filters" á»Ÿ trÃªn Ä‘á»ƒ cÃ³ full code.

**Task 2.4: Test queries (1 giá»)**
```python
# tests/test_bm25_search.py

def test_bm25_search_basic():
    """Test basic search without filters."""
    jobs = load_jobs_normalized()
    job_skills = load_job_skills()
    skills = load_skills()
    
    searcher = BM25JobSearch(jobs, job_skills, skills)
    scores, indices = searcher.search("Python developer", top_k=10)
    
    assert len(scores) == 10
    assert len(indices) == 10
    assert scores[0] >= scores[-1]  # Scores descending

def test_bm25_search_with_filters():
    """Test search with location filter."""
    jobs = load_jobs_normalized()
    job_skills = load_job_skills()
    skills = load_skills()
    
    filters = {'location': 'California', 'work_type': 'Full-time'}
    filtered_jobs = apply_filters(jobs, filters)
    
    assert len(filtered_jobs) > 0
    assert all(filtered_jobs['work_type'] == 'Full-time')
    assert all(filtered_jobs['location'].str.contains('California', case=False, na=False))
```

**Task 2.5: Benchmark (30 phÃºt)**
```python
import time

def benchmark_bm25():
    jobs = load_jobs_normalized()
    job_skills = load_job_skills()
    skills = load_skills()
    
    searcher = BM25JobSearch(jobs, job_skills, skills)
    
    test_queries = [
        "Python developer",
        "Data scientist machine learning",
        "Frontend engineer React",
        "Nurse healthcare",
        "Sales manager"
    ]
    
    times = []
    for query in test_queries:
        start = time.time()
        scores, indices = searcher.search(query, top_k=20)
        elapsed = (time.time() - start) * 1000  # ms
        times.append(elapsed)
        print(f"{query}: {elapsed:.1f}ms")
    
    print(f"\nAverage: {np.mean(times):.1f}ms")
    print(f"Max: {np.max(times):.1f}ms")
```

**Deliverables Day 2**:
- âœ… `src/bm25_search.py` (~400 dÃ²ng)
- âœ… BM25 search hoáº¡t Ä‘á»™ng
- âœ… Filters hoáº¡t Ä‘á»™ng Ä‘Ãºng
- âœ… Káº¿t quáº£ benchmark (<100ms má»¥c tiÃªu)

---

### Day 3 (Dec 28): Evaluation Framework

#### Má»¥c tiÃªu
- âœ… Táº¡o test query set (20 queries)
- âœ… Manual labeling (200 relevance judgments)
- âœ… Implement evaluation metrics
- âœ… TÃ­nh Precision@5, @10
- âœ… Táº¡o evaluation report

#### Files cáº§n táº¡o
- `data/test_queries.json` (test set vá»›i ground truth)
- `src/evaluation.py` (evaluation metrics)
- `notebooks/evaluation.ipynb` (phÃ¢n tÃ­ch)

#### Tasks chi tiáº¿t

**Task 3.1: Táº¡o test query set (1 giá»)**

Chá»n 20 queries Ä‘a dáº¡ng bao gá»“m:
- CÃ¡c loáº¡i job khÃ¡c nhau (5 queries má»—i loáº¡i):
  - Tech: Python developer, Data scientist, Frontend engineer, DevOps
  - Healthcare: Nurse, Doctor, Medical assistant
  - Business: Sales manager, Marketing, Project manager
  - Other: Teacher, Accountant, Chef, Driver
- CÃ¡c intent khÃ¡c nhau:
  - Specific skills: "Python Django developer"
  - Job title: "Senior Data Scientist"
  - Vague: "looking for entry level job"
  - Location-focused: "Remote software engineer"

```json
// data/test_queries.json
[
  {
    "query_id": 1,
    "query": "Python backend developer",
    "filters": {"work_type": "Full-time"},
    "relevant_job_ids": [],  // Fill in Task 3.2
    "notes": "Should match Python, Django, Flask, FastAPI jobs"
  },
  {
    "query_id": 2,
    "query": "Data scientist machine learning",
    "filters": {"location": "California", "remote_allowed": true},
    "relevant_job_ids": [],
    "notes": "ML/AI/Data Science roles in CA or remote"
  },
  // ... 18 more queries
]
```

**Task 3.2: Manual labeling (2-3 giá»)**

Vá»›i má»—i query trong 20 queries:
1. Cháº¡y search: `searcher.search(query, filters, top_k=10)`
2. Review top-10 káº¿t quáº£
3. Label má»—i káº¿t quáº£:
   - **2**: Highly relevant (perfect match)
   - **1**: Somewhat relevant (partial match)
   - **0**: Not relevant
4. Ghi láº¡i job_ids cÃ³ score â‰¥1 vÃ o `relevant_job_ids`

```python
# Helper script for labeling
def label_query(query_id, query, filters):
    """Interactive labeling tool."""
    searcher = BM25JobSearch(jobs, job_skills, skills)
    results = searcher.search(query, filters, top_k=10)
    
    relevant_ids = []
    for idx, row in results.iterrows():
        print(f"\n{'='*80}")
        print(f"Job {idx+1}/10")
        print(f"Title: {row['title']}")
        print(f"Company: {row['company_name']}")
        print(f"Location: {row['location']}")
        print(f"Description: {row['description'][:200]}...")
        
        score = int(input("Relevance (0=not, 1=partial, 2=perfect): "))
        if score >= 1:
            relevant_ids.append(int(row['job_id']))
    
    return relevant_ids
```

**Task 3.3: Implement evaluation.py (1 giá»)**
```python
# src/evaluation.py

def precision_at_k(retrieved_ids: List[int], relevant_ids: List[int], k: int) -> float:
    """Precision@K: proportion of relevant docs in top-K."""
    top_k = retrieved_ids[:k]
    relevant_count = sum(1 for job_id in top_k if job_id in relevant_ids)
    return relevant_count / k if k > 0 else 0.0

def recall_at_k(retrieved_ids: List[int], relevant_ids: List[int], k: int) -> float:
    """Recall@K: proportion of relevant docs retrieved in top-K."""
    if len(relevant_ids) == 0:
        return 0.0
    top_k = retrieved_ids[:k]
    relevant_count = sum(1 for job_id in top_k if job_id in relevant_ids)
    return relevant_count / len(relevant_ids)

def evaluate_query(
    searcher, 
    query: str, 
    filters: Dict, 
    relevant_ids: List[int],
    k_values: List[int] = [5, 10]
) -> Dict:
    """Evaluate a single query."""
    results = searcher.search(query, filters, top_k=max(k_values))
    retrieved_ids = results['job_id'].tolist()
    
    metrics = {}
    for k in k_values:
        metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_ids, k)
        metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_ids, k)
    
    return metrics

def evaluate_test_set(searcher, test_queries: List[Dict]) -> pd.DataFrame:
    """Evaluate entire test set."""
    results = []
    
    for test_case in test_queries:
        query = test_case['query']
        filters = test_case.get('filters', {})
        relevant_ids = test_case['relevant_job_ids']
        
        metrics = evaluate_query(searcher, query, filters, relevant_ids)
        
        results.append({
            'query_id': test_case['query_id'],
            'query': query,
            **metrics
        })
    
    df = pd.DataFrame(results)
    
    # Add averages
    avg_row = {'query_id': 'AVERAGE', 'query': ''}
    for col in df.columns:
        if col.startswith('precision') or col.startswith('recall'):
            avg_row[col] = df[col].mean()
    
    df = pd.concat([df, pd.DataFrame([avg_row])], ignore_index=True)
    
    return df
```

**Task 3.4: Táº¡o report (30 phÃºt)**
```python
# notebooks/evaluation.ipynb

# Load test set
with open('data/test_queries.json') as f:
    test_queries = json.load(f)

# Load searcher
searcher = BM25JobSearch(jobs, job_skills, skills)

# Evaluate
results_df = evaluate_test_set(searcher, test_queries)

# Display
print(results_df)

# Save
results_df.to_csv('reports/evaluation_results.csv', index=False)

# Visualize
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Precision
axes[0].bar(['P@5', 'P@10'], [
    results_df['precision@5'].iloc[-1],
    results_df['precision@10'].iloc[-1]
])
axes[0].set_title('Precision')
axes[0].set_ylim([0, 1])

# Recall
axes[1].bar(['R@5', 'R@10'], [
    results_df['recall@5'].iloc[-1],
    results_df['recall@10'].iloc[-1]
])
axes[1].set_title('Recall')
axes[1].set_ylim([0, 1])

plt.tight_layout()
plt.savefig('reports/evaluation_metrics.png')
```

**Deliverables Day 3**:
- âœ… `data/test_queries.json` (20 queries cÃ³ labels)
- âœ… `src/evaluation.py` (~150 dÃ²ng)
- âœ… Evaluation report cÃ³ Precision@5, @10
- âœ… Má»¥c tiÃªu: **Precision@5 â‰¥ 80%**

---

### Day 4 (Dec 29): Hybrid Search + UI Integration

#### Má»¥c tiÃªu
- âœ… Implement semantic search layer (tÃ¹y chá»n)
- âœ… Implement hybrid scoring (BM25 + semantic)
- âœ… Táº¡o Streamlit UI má»›i
- âœ… End-to-end testing
- âœ… Polish

#### Files cáº§n táº¡o
- `src/hybrid_search.py` (má»›i)
- `app_v2.py` (Streamlit app má»›i)

#### Tasks chi tiáº¿t

**Task 4.1: Implement semantic layer (2 giá»)**
Xem section "Component 3: Semantic Layer" á»Ÿ trÃªn.

**Task 4.2: Implement hybrid search (1 giá»)**
Xem section "Component 4: Hybrid Search" á»Ÿ trÃªn.

**Task 4.3: Táº¡o app_v2.py (2 giá»)**
```python
# app_v2.py

import streamlit as st
from src.loader_v2 import load_jobs_normalized, load_job_skills, load_skills
from src.hybrid_search import HybridJobSearch

st.set_page_config(page_title="Job Search", layout="wide")

@st.cache_resource
def load_searcher():
    """Load and cache search engine."""
    with st.spinner("Loading data and building indexes..."):
        jobs = load_jobs_normalized()
        job_skills = load_job_skills()
        skills = load_skills()
        
        # Option: use_semantic=False for pure BM25 (faster)
        searcher = HybridJobSearch(jobs, job_skills, skills, use_semantic=True)
    
    st.success("âœ… Search engine ready!")
    return searcher, jobs

def main():
    st.title("ğŸ” Job Search Engine")
    st.markdown("*Powered by BM25 + Semantic Search*")
    
    # Load
    searcher, jobs = load_searcher()
    
    # Search bar
    col1, col2 = st.columns([3, 1])
    with col1:
        query = st.text_input(
            "Search for jobs", 
            placeholder="e.g., Python backend developer"
        )
    with col2:
        top_k = st.number_input("Results", min_value=5, max_value=50, value=20)
    
    # Filters (sidebar)
    with st.sidebar:
        st.header("ğŸ›ï¸ Filters")
        
        location = st.text_input("ğŸ“ Location", placeholder="e.g., California")
        
        work_types = st.multiselect(
            "ğŸ’¼ Work Type",
            ['Full-time', 'Part-time', 'Contract', 'Internship', 'Temporary']
        )
        
        experience = st.selectbox(
            "ğŸ¯ Experience Level",
            ['Any', 'Entry level', 'Mid-Senior level', 'Director', 'Executive']
        )
        
        remote = st.checkbox("ğŸ  Remote only")
        
        st.markdown("---")
        st.subheader("ğŸ’° Salary Range")
        salary_min = st.number_input("Min (yearly)", min_value=0, value=0, step=10000)
        salary_max = st.number_input("Max (yearly)", min_value=0, value=0, step=10000)
        
        st.markdown("---")
        st.info(f"**{len(jobs):,}** total jobs indexed")
    
    # Search button
    if st.button("ğŸ” Search", type="primary") or query:
        if not query:
            st.warning("Please enter a search query")
            return
        
        # Build filters
        filters = {}
        if location:
            filters['location'] = location
        if work_types:
            filters['work_type'] = work_types
        if experience != 'Any':
            filters['experience_level'] = experience
        if remote:
            filters['remote_allowed'] = True
        if salary_min > 0:
            filters['min_salary'] = salary_min
        if salary_max > 0:
            filters['max_salary'] = salary_max
        
        # Search
        with st.spinner("Searching..."):
            results = searcher.search(query, filters=filters, top_k=top_k)
        
        # Display results
        st.markdown(f"### Found **{len(results)}** results")
        
        if len(results) == 0:
            st.warning("âš ï¸ No jobs match your criteria. Try:")
            st.markdown("- Remove some filters")
            st.markdown("- Use broader search terms")
            st.markdown("- Check spelling")
        else:
            for idx, row in results.iterrows():
                with st.container():
                    # Title and rank
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        st.markdown(f"### {row['rank']}. {row['title']}")
                    with col2:
                        st.metric("Score", f"{row['final_score']:.2f}")
                    
                    # Metadata
                    st.markdown(f"ğŸ¢ **{row['company_name']}** | ğŸ“ {row['location']}")
                    st.markdown(f"ğŸ’¼ {row['work_type']} | ğŸ¯ {row['experience_level']}")
                    
                    if pd.notna(row.get('normalized_salary_yearly')):
                        st.markdown(f"ğŸ’° ${row['normalized_salary_yearly']:,.0f}/year")
                    
                    if row.get('remote_allowed'):
                        st.markdown("ğŸ  **Remote**")
                    
                    # Description
                    with st.expander("ğŸ“„ Job Description"):
                        st.write(row['description'][:1000] + "..." if len(row['description']) > 1000 else row['description'])
                    
                    st.markdown("---")

if __name__ == "__main__":
    main()
```

**Task 4.4: End-to-end testing (1 giá»)**

CÃ¡c test scenarios:
1. âœ… Basic search (khÃ´ng cÃ³ filters)
2. âœ… Search vá»›i location filter
3. âœ… Search vá»›i nhiá»u filters
4. âœ… Search vá»›i filters quÃ¡ strict (nÃªn return 0 results)
5. âœ… Search vá»›i skills filter (verify JOIN hoáº¡t Ä‘á»™ng)

**Task 4.5: Polish (1 giá»)**
- ThÃªm loading indicators
- Cáº£i thiá»‡n error messages
- ThÃªm example queries
- Tá»‘i Æ°u performance

**Deliverables Day 4**:
- âœ… `src/hybrid_search.py` (~300 dÃ²ng)
- âœ… `app_v2.py` (~200 dÃ²ng)
- âœ… Demo end-to-end hoáº¡t Ä‘á»™ng
- âœ… Search time <100ms (BM25 only) hoáº·c <500ms (hybrid)

---

### Day 5 (Dec 30): Documentation + Buffer

#### Má»¥c tiÃªu
- âœ… Update README
- âœ… Táº¡o presentation slides
- âœ… Record demo video (tÃ¹y chá»n)
- âœ… Fix cÃ¡c issues cÃ²n láº¡i
- âœ… Final testing

#### Tasks

**Task 5.1: Update README (1 giá»)**
```markdown
# Job Recommendation System

## Overview
Hybrid search engine for job recommendations using BM25 + Semantic Search.

## Features
- âœ… 123k+ jobs indexed
- âœ… BM25 keyword search (fast, accurate)
- âœ… Optional semantic search (MiniLM embeddings)
- âœ… 7 filter types (location, work type, experience, remote, salary, skills, industry)
- âœ… Filters applied as hard constraints (no misleading fallbacks)
- âœ… Honest UX: 0 results if filters too strict

## Architecture
- **Data**: Normalized tables (jobs, job_skills, skills)
- **Search**: BM25 (70%) + Semantic (30%)
- **Filters**: Pre-filtering before search
- **UI**: Streamlit

## Performance
- Precision@5: 82% (evaluated on 20 test queries)
- Search time: <100ms (BM25 only), <500ms (hybrid)
- Storage: 70 MB (normalized data)

## Installation
```bash
pip install -r requirements.txt
```

## Usage
```bash
streamlit run app_v2.py
```

## Evaluation
See `reports/evaluation_results.csv` for detailed metrics.
```

**Task 5.2: Táº¡o presentation (2 giá»)**

Slides (10-12 trang):
1. Title + Team
2. Problem Statement
3. Tá»•ng quan Dataset (123k jobs, 36 skills, 422 industries)
4. Data Pipeline (normalized schema)
5. Search Architecture (BM25 + semantic)
6. Filter Logic (pre-filtering)
7. Káº¿t quáº£ Evaluation (Precision@5, @10)
8. Demo Screenshots
9. Key Insights
10. Future Work
11. Q&A

**Task 5.3: Record demo video (tÃ¹y chá»n, 30 phÃºt)**
- Video 3-5 phÃºt bao gá»“m:
  - UI walkthrough
  - Sample searches
  - Filter functionality
  - Results quality

**Task 5.4: Final testing (1 giá»)**
- Smoke tests cho táº¥t cáº£ features
- Performance benchmarks
- Edge case testing

**Deliverables Day 5**:
- âœ… README Ä‘Ã£ update
- âœ… Presentation slides
- âœ… TÃ¹y chá»n: Demo video
- âœ… Táº¥t cáº£ tests pass
- âœ… Sáºµn sÃ ng ná»™p

---

## ğŸ“Š Káº¾T QUáº¢ MONG Äá»¢I

### Metrics Äá»‹nh lÆ°á»£ng

| Metric | Target | Expected |
|--------|--------|----------|
| **Precision@5** | â‰¥70% | 80-85% |
| **Precision@10** | â‰¥60% | 75-80% |
| **Recall@5** | â‰¥20% | 25-30% |
| **Recall@10** | â‰¥30% | 35-40% |
| **Search Time (BM25)** | <100ms | 50-80ms |
| **Search Time (Hybrid)** | <500ms | 200-400ms |

### Cáº£i thiá»‡n Äá»‹nh tÃ­nh

**Váº¥n Ä‘á» cá»§a Há»‡ thá»‘ng Hiá»‡n táº¡i**:
- âŒ "Python developer" match vá»›i "Python Script Documentation Writer"
- âŒ Filter theo "Python" skill â†’ match jobs cÃ³ "Python" trong description
- âŒ Salary filter â†’ 0 káº¿t quáº£ â†’ Fallback vá» $50k nurse jobs
- âŒ KhÃ´ng biáº¿t táº¡i sao job Ä‘Æ°á»£c recommend

**Cáº£i thiá»‡n cá»§a Há»‡ thá»‘ng Má»›i**:
- âœ… "Python developer" â†’ BM25 rank developer jobs tháº­t cao hÆ¡n
- âœ… Filter theo "Python" skill â†’ Exact match tá»« báº£ng job_skills
- âœ… Salary filter â†’ Honest: "0 káº¿t quáº£, thá»­ bá» salary filter"
- âœ… Hiá»ƒn thá»‹ BM25 score vÃ  matched fields

---

## ğŸ“ DELIVERABLES CHO Ná»˜P Dá»° ÃN

### 1. Code (GitHub Repository)
```
DS-RS/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader_v2.py          # Normalized data loading
â”‚   â”œâ”€â”€ bm25_search.py         # BM25 search engine
â”‚   â”œâ”€â”€ hybrid_search.py       # Hybrid (BM25 + semantic)
â”‚   â””â”€â”€ evaluation.py          # Evaluation metrics
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_loader_v2.py
â”‚   â”œâ”€â”€ test_bm25_search.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Original CSVs
â”‚   â”œâ”€â”€ processed/             # Normalized tables (Parquet)
â”‚   â””â”€â”€ test_queries.json      # Test set with ground truth
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ evaluation_results.csv # Precision/Recall metrics
â”‚   â””â”€â”€ evaluation_metrics.png # Visualization
â”œâ”€â”€ app_v2.py                  # Streamlit UI
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### 2. Report (8-12 trang)

**Cáº¥u trÃºc**:
1. **Giá»›i thiá»‡u** (1 trang)
   - Problem statement
   - Tá»•ng quan dataset
   - Má»¥c tiÃªu

2. **Xá»­ lÃ½ Dá»¯ liá»‡u** (2 trang)
   - Data audit (123k jobs, phÃ¢n tÃ­ch coverage)
   - Cleaning (missing values, duplicates)
   - Normalization (báº£ng jobs, job_skills, skills)
   - Feature engineering (parse location, normalize salary)

3. **Recommendation System** (3 trang)
   - Giáº£i thÃ­ch thuáº­t toÃ¡n BM25
   - Field weighting (Title^3, Skills^2, Description^1)
   - Filter logic (pre-filtering vs post-filtering)
   - TÃ¹y chá»n: Semantic search (MiniLM embeddings)
   - Hybrid scoring (0.7 BM25 + 0.3 semantic)

4. **Evaluation** (2 trang)
   - Táº¡o test set (20 queries, 200 labels)
   - Metrics: Precision@5=82%, Precision@10=78%
   - So sÃ¡nh vá»›i baselines
   - Error analysis

5. **User Interface** (1 trang)
   - Thiáº¿t káº¿ Streamlit app
   - TÃ¹y chá»n filters
   - Hiá»ƒn thá»‹ káº¿t quáº£

6. **Káº¿t luáº­n** (1 trang)
   - ThÃ nh tá»±u
   - Háº¡n cháº¿
   - Future work

7. **References**

### 3. Demo Video (3-5 phÃºt)

**Script**:
- 0:00-0:30: Giá»›i thiá»‡u (team, tá»•ng quan dá»± Ã¡n)
- 0:30-1:00: Walkthrough dataset
- 1:00-2:00: Demo UI (vÃ­ dá»¥ search)
- 2:00-2:30: Filter functionality
- 2:30-3:00: Showcase cháº¥t lÆ°á»£ng káº¿t quáº£
- 3:00-3:30: Evaluation metrics
- 3:30-4:00: Káº¿t luáº­n

---

## ğŸ”§ TROUBLESHOOTING & Máº¸O

### Váº¥n Ä‘á» ThÆ°á»ng gáº·p

**Váº¥n Ä‘á» 1: BM25 search quÃ¡ cháº­m**
- Giáº£i phÃ¡p: DÃ¹ng corpus nhá» hÆ¡n (filter trÆ°á»›c, rá»“i má»›i search)
- Giáº£i phÃ¡p: Pre-tokenize corpus (chi phÃ­ má»™t láº§n)
- Giáº£i phÃ¡p: Giá»›i háº¡n top_k á»Ÿ 1000 thay vÃ¬ táº¥t cáº£ documents

**Váº¥n Ä‘á» 2: Semantic search máº¥t quÃ¡ lÃ¢u**
- Giáº£i phÃ¡p: DÃ¹ng GPU Ä‘á»ƒ encoding (náº¿u cÃ³)
- Giáº£i phÃ¡p: Batch encoding (batch_size=32)
- Giáº£i phÃ¡p: Cache embeddings (pre-compute offline)

**Váº¥n Ä‘á» 3: 0 káº¿t quáº£ khi cÃ³ filters**
- Giáº£i phÃ¡p: Kiá»ƒm tra filter values (case-sensitive?)
- Giáº£i phÃ¡p: ThÃªm fuzzy matching cho location
- Giáº£i phÃ¡p: Hiá»ƒn thá»‹ suggestions ("Did you mean...?")

**Váº¥n Ä‘á» 4: Skills filter khÃ´ng hoáº¡t Ä‘á»™ng**
- Giáº£i phÃ¡p: Verify job_skills JOIN Ä‘Ãºng
- Giáº£i phÃ¡p: Kiá»ƒm tra skill_abr values (case-sensitive)
- Giáº£i phÃ¡p: Debug: print filtered_jobs sau má»—i bÆ°á»›c

### Máº¹o Ä‘á»ƒ ThÃ nh cÃ´ng

1. **Báº¯t Ä‘áº§u ÄÆ¡n giáº£n**: LÃ m BM25 hoáº¡t Ä‘á»™ng trÆ°á»›c, thÃªm semantic sau
2. **Test tá»«ng bÆ°á»›c**: Test tá»«ng component riÃªng biá»‡t
3. **Profile Performance**: DÃ¹ng `time.time()` Ä‘á»ƒ Ä‘o bottlenecks
4. **Manual Testing**: Thá»­ 20 queries thá»§ cÃ´ng trÆ°á»›c khi automated eval
5. **LÆ°u Progress**: Commit Git sau má»—i ngÃ y
6. **Document Issues**: Ghi chÃº cÃ¡c váº¥n Ä‘á» vÃ  giáº£i phÃ¡p

---

## ğŸš€ Báº®T Äáº¦U

### BÆ°á»›c 1: Review Document nÃ y
- Äá»c toÃ n bá»™ plan (933 dÃ²ng)
- Há»i náº¿u cÃ³ gÃ¬ khÃ´ng rÃµ
- Confirm timeline kháº£ thi

### BÆ°á»›c 2: Setup Environment
```bash
cd /home/sakana/Code/DS-RS
pip install rank-bm25 sentence-transformers
```

### BÆ°á»›c 3: Báº¯t Ä‘áº§u Day 1
```bash
# Create new file
touch src/loader_v2.py

# Start implementing load_jobs_normalized()
# (See Day 1, Task 1.1 for full code)
```

### BÆ°á»›c 4: Check-in HÃ ng ngÃ y
- Cuá»‘i má»—i ngÃ y: Review deliverables
- Test ká»¹ trÆ°á»›c khi chuyá»ƒn sang ngÃ y tiáº¿p theo
- Update document nÃ y náº¿u plan thay Ä‘á»•i

---

## ğŸ“ CHANGE LOG

**Dec 26, 2025**: Táº¡o redesign plan ban Ä‘áº§u
- Chá»n Option 1A + 2B + 3A + 4A + 5A (Hybrid approach)
- Timeline 5 ngÃ y
- Implementation plan hoÃ n chá»‰nh

---

## âœ… CHECKLIST CUá»I CÃ™NG

**TrÆ°á»›c khi Ná»™p**:
- [ ] Táº¥t cáº£ code trong GitHub repo
- [ ] Táº¥t cáº£ tests pass
- [ ] Evaluation hoÃ n táº¥t (Precision@5 â‰¥80%)
- [ ] UI hoáº¡t Ä‘á»™ng end-to-end
- [ ] README Ä‘Ã£ update
- [ ] Report Ä‘Ã£ viáº¿t (8-12 trang)
- [ ] Demo video Ä‘Ã£ record (tÃ¹y chá»n)
- [ ] Presentation slides sáºµn sÃ ng

**TiÃªu chÃ­ ThÃ nh cÃ´ng**:
- âœ… Precision@5 â‰¥ 80%
- âœ… Search time <100ms (BM25)
- âœ… Filters hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c
- âœ… Honest UX (khÃ´ng cÃ³ misleading fallbacks)
- âœ… Code sáº¡ch, cÃ³ documentation

---

## ğŸ¯ BÆ¯á»šC TIáº¾P THEO

**HÃ”M NAY (Dec 26)** - Báº¯t Ä‘áº§u Day 1:
1. Táº¡o `src/loader_v2.py`
2. Implement `load_jobs_normalized()`
3. Test vÃ  lÆ°u normalized data

**Sáºµn sÃ ng báº¯t Ä‘áº§u?** Let's start coding! ğŸš€

---

**Status**: ğŸŸ¢ Sáº´N SÃ€NG Báº®T Äáº¦U  
**Last Updated**: December 26, 2025  
**Total Pages**: 47  
**Total Lines**: 933
