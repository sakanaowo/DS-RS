# PhÃ¢n TÃ­ch Váº¥n Äá» & Káº¿ Hoáº¡ch Triá»ƒn Khai Láº¡i

**Date**: December 26, 2025  
**Status**: ğŸ”´ CRITICAL - Cáº§n thiáº¿t káº¿ láº¡i tá»« Ä‘áº§u

---

## ğŸš¨ Váº¤N Äá»€ PHÃT HIá»†N

### 1. Váº¥n Ä‘á» vá» Data Pipeline

#### **A. Thiáº¿t káº¿ sai vá» cáº¥u trÃºc dá»¯ liá»‡u**

**Hiá»‡n táº¡i lÃ m gÃ¬**:
```python
# loader.py - build_enriched_jobs()
1. Load postings.csv (123k jobs)
2. JOIN vá»›i job_skills.csv â†’ aggregate thÃ nh "skill1, skill2, skill3"
3. JOIN vá»›i job_industries.csv â†’ aggregate thÃ nh "industry1, industry2"
4. JOIN vá»›i salaries.csv
5. JOIN vá»›i benefits.csv
6. JOIN vá»›i companies.csv (company metadata)
â†’ Táº¡o má»™t DataFrame ráº¥t rá»™ng vá»›i nhiá»u cá»™t text Ä‘Ã£ aggregated
```

**Váº¥n Ä‘á»**:
- âŒ **Máº¥t cáº¥u trÃºc quan há»‡**: Skills vÃ  industries bá»‹ gá»™p thÃ nh string â†’ máº¥t kháº£ nÄƒng filter chÃ­nh xÃ¡c
- âŒ **KhÃ´ng thá»ƒ search theo skill riÃªng láº»**: "Python, Java, SQL" â†’ user search "Python" sáº½ match cáº£ string
- âŒ **Duplicate data**: Má»—i job cÃ³ nhiá»u skills â†’ gá»™p thÃ nh string â†’ láº·p láº¡i nhiá»u láº§n
- âŒ **Preprocessing khÃ´ng phÃ¹ há»£p**: Vector hÃ³a trÃªn text Ä‘Ã£ gá»™p â†’ bias vá» jobs cÃ³ nhiá»u skills

**VÃ­ dá»¥ cá»¥ thá»ƒ**:
```
Job A: skills = "Python, Machine Learning, SQL, Data Analysis, Statistics"
Job B: skills = "Python"

User query: "Python developer"
â†’ Job A cÃ³ score cao hÆ¡n vÃ¬ cÃ³ nhiá»u keywords
â†’ NhÆ°ng Job B cÃ³ thá»ƒ relevant hÆ¡n (chá»‰ cáº§n Python)
```

#### **B. Thiáº¿t káº¿ sai vá» feature engineering**

**Hiá»‡n táº¡i lÃ m gÃ¬**:
```python
# preprocessing.py - prepare_features()
content = title*2 + " " + description + " " + skills_desc
â†’ Vector hÃ³a content nÃ y vá»›i TF-IDF hoáº·c MiniLM
```

**Váº¥n Ä‘á»**:
- âŒ **Title Ä‘Æ°á»£c repeat 2 láº§n**: Artificial weighting khÃ´ng dá»±a trÃªn data
- âŒ **Description quÃ¡ dÃ i**: ThÆ°á»ng 500-2000 chars â†’ overwhelm title/skills
- âŒ **KhÃ´ng cÃ³ skill weighting**: Skill quan trá»ng nhÆ°ng bá»‹ drown bá»Ÿi description
- âŒ **KhÃ´ng cÃ³ industry context**: Industry/company size bá»‹ bá» qua

**Káº¿t quáº£**:
- Search "Python developer" â†’ match cáº£ jobs vá» "Python library documentation" (description cÃ³ Python)
- Filter by skill khÃ´ng work vÃ¬ skills Ä‘Ã£ bá»‹ gá»™p vÃ o content text

---

### 2. Váº¥n Ä‘á» vá» Recommendation Logic

#### **A. Pure content-based khÃ´ng Ä‘á»§**

**Hiá»‡n táº¡i**:
- TF-IDF/MiniLM trÃªn text content
- Cosine similarity Ä‘á»ƒ rank
- Apply filters sau khi search

**Váº¥n Ä‘á»**:
- âŒ **Cold start**: User má»›i khÃ´ng cÃ³ history â†’ chá»‰ dá»±a vÃ o query text
- âŒ **KhÃ´ng cÃ³ personalization**: Táº¥t cáº£ users search "Python" Ä‘á»u cÃ³ káº¿t quáº£ giá»‘ng nhau
- âŒ **KhÃ´ng cÃ³ context**: Location, experience level, salary expectations khÃ´ng Ä‘Æ°á»£c model há»c

**Thiáº¿u**:
- Collaborative filtering (users who viewed A also viewed B)
- Hybrid approach (content + collaborative + popularity)
- Learning-to-rank (train model tá»« click/apply data)

#### **B. Filter logic khÃ´ng tá»‘i Æ°u**

**Hiá»‡n táº¡i**:
```python
# Progressive fallback (7 layers)
Layer 1: All filters
Layer 2: Remove salary
Layer 3: Remove experience
...
Layer 7: Popular jobs
```

**Váº¥n Ä‘á»**:
- âŒ **Band-aid solution**: Fallback lÃ  workaround cho data quality kÃ©m
- âŒ **KhÃ´ng giáº£i quyáº¿t root cause**: Data pipeline sai tá»« Ä‘áº§u
- âŒ **User experience kÃ©m**: User search vá»›i filters cá»¥ thá»ƒ â†’ nháº­n Ä‘Æ°á»£c unrelated jobs (Layer 7)

**VÃ­ dá»¥**:
```
User: "Remote Python developer in California, $100k-150k"
â†’ Layer 1: 0 results (salary data 23% coverage)
â†’ Layer 2: 0 results (remove salary)
â†’ ...
â†’ Layer 7: Popular jobs (cÃ³ thá»ƒ lÃ  "Nurse in Texas, $50k")
âŒ Completely irrelevant!
```

---

### 3. Váº¥n Ä‘á» vá» Data Quality

#### **A. Missing data khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Ãºng**

**Hiá»‡n táº¡i**:
- Smart imputation (data_quality.py)
- Infer work_type, location, experience tá»« patterns

**Váº¥n Ä‘á»**:
- âŒ **Imputation khÃ´ng reliable**: "Senior" in title â†’ "Mid-Senior level" (khÃ´ng pháº£i lÃºc nÃ o cÅ©ng Ä‘Ãºng)
- âŒ **Salary khÃ´ng impute**: 23% coverage â†’ filters khÃ´ng work
- âŒ **Remote flag 12% coverage**: Majority jobs khÃ´ng biáº¿t remote hay khÃ´ng

**Root cause**: KhÃ´ng nÃªn impute tá»« patterns â†’ nÃªn fix data source hoáº·c accept missing

#### **B. Text processing khÃ´ng phÃ¹ há»£p**

**Hiá»‡n táº¡i**:
```python
clean_text():
- Remove HTML, URLs, special chars
- Lowercase
- Remove stopwords
- Join vá»›i spaces
```

**Váº¥n Ä‘á»**:
- âŒ **Máº¥t semantic**: "Machine Learning" â†’ "machine learning" â†’ tokenize â†’ "machine" + "learning"
- âŒ **KhÃ´ng cÃ³ n-grams**: "Data Scientist" bá»‹ tÃ¡ch thÃ nh "data" + "scientist"
- âŒ **Skill names bá»‹ mangled**: "C++" â†’ "c" (remove special chars)

---

## ğŸ¯ YÃŠU Cáº¦U THá»°C Sá»° Cá»¦A PROJECT

### Tá»« FinalProject_recommendation_system.md

**BÃ i toÃ¡n**: XÃ¢y dá»±ng há»‡ thá»‘ng gá»£i Ã½ viá»‡c lÃ m

**YÃªu cáº§u core**:
1. âœ… Dataset â‰¥ 2,000 items (cÃ³: 123k jobs)
2. âœ… â‰¥ 5 features (cÃ³: title, description, skills, industry, company, location, salary, work_type, experience)
3. âŒ **Recommendation quality** - CHÆ¯A Äáº T
4. âŒ **Filter support** - CHÆ¯A WORK ÄÃšNG
5. âœ… UI (Streamlit)

**YÃªu cáº§u data processing**:
1. âœ… Missing values (Ä‘Ã£ xá»­ lÃ½ nhÆ°ng sai cÃ¡ch)
2. âŒ **Chuáº©n hÃ³a dá»¯ liá»‡u** - CHÆ¯A ÄÃšNG
3. âœ… Loáº¡i bá» duplicates
4. âš ï¸ **Vector hÃ³a** - ÄANG LÃ€M SAI

**YÃªu cáº§u evaluation**:
1. âŒ Precision@K, Recall@K - CHÆ¯A CÃ“ GROUND TRUTH
2. âŒ User study - CHÆ¯A CÃ“

**NÃ¢ng cao**:
1. âš ï¸ Context-aware - CÃ³ progressive fallback nhÆ°ng khÃ´ng pháº£i context-aware tháº­t
2. âœ… Real-time - Search <50ms
3. âœ… User history tracking
4. âŒ **Advanced ML** - CHÆ¯A CÃ“

---

## ğŸ’¡ Ã TÆ¯á»NG Má»šI - THIáº¾T Káº¾ Láº I Tá»ª Äáº¦U

### Approach 1: Multi-Index Search (Recommended)

**Idea**: Thay vÃ¬ vector hÃ³a toÃ n bá»™ job thÃ nh 1 vector, tÃ¡ch thÃ nh nhiá»u indexes:

```
Index 1: Title + Description (semantic meaning)
Index 2: Skills (exact match + embedding)
Index 3: Industry + Company (categorical)
Index 4: Location (geo-spatial or string)
Index 5: Metadata (salary, work_type, experience, remote)
```

**Search flow**:
```python
def search(query, filters):
    # 1. Parse query intent
    query_parsed = parse_query(query)  # "Python dev in SF" â†’ {keywords: [python, dev], location: SF}
    
    # 2. Multi-stage retrieval
    candidates_title = index1.search(query_parsed['keywords'], top_k=1000)
    candidates_skills = index2.search(query_parsed['skills'], top_k=1000)
    
    # 3. Merge & rerank
    candidates = merge(candidates_title, candidates_skills, weights=[0.6, 0.4])
    
    # 4. Apply filters (hard constraints)
    filtered = apply_filters(candidates, filters)
    
    # 5. Final ranking (learning-to-rank or heuristic)
    ranked = rank(filtered, user_profile=None)  # Placeholder for personalization
    
    return ranked[:top_k]
```

**Advantages**:
- âœ… Skills Ä‘Æ°á»£c index riÃªng â†’ filter chÃ­nh xÃ¡c
- âœ… CÃ³ thá»ƒ weight tá»«ng component
- âœ… Filter work properly (hard constraints)
- âœ… Dá»… debug (biáº¿t score tá»« Ä‘Ã¢u)

**Disadvantages**:
- âš ï¸ Phá»©c táº¡p hÆ¡n (nhiá»u indexes)
- âš ï¸ Cáº§n tuning weights

---

### Approach 2: Hybrid Embeddings (Advanced)

**Idea**: Sá»­ dá»¥ng multiple embedding models cho tá»«ng loáº¡i content:

```
Model 1: all-MiniLM-L6-v2 cho title + description (384 dims)
Model 2: SkillBERT hoáº·c fine-tuned model cho skills (128 dims)
Model 3: GeoEncoder cho location (64 dims)
â†’ Concat = 576 dims vector cho má»—i job
```

**Search flow**:
```python
def search(query, filters):
    # 1. Encode query vá»›i multi-models
    query_vec = concat([
        model1.encode(query_text),
        model2.encode(query_skills),
        model3.encode(query_location)
    ])
    
    # 2. FAISS search
    candidates = faiss_index.search(query_vec, top_k=1000)
    
    # 3. Apply filters
    filtered = apply_filters(candidates, filters)
    
    return filtered[:top_k]
```

**Advantages**:
- âœ… End-to-end learned
- âœ… CÃ³ thá»ƒ fine-tune models
- âœ… State-of-the-art approach

**Disadvantages**:
- âŒ Cáº§n data Ä‘á»ƒ fine-tune
- âŒ Computationally expensive
- âŒ KhÃ³ debug

---

### Approach 3: Simplified BM25 + Filters (Pragmatic)

**Idea**: Quay láº¡i basics - BM25 vá»›i filters Ä‘Ãºng cÃ¡ch

```
1. Index jobs vá»›i Elasticsearch/Whoosh/Custom BM25
2. Query = title + description + skills (khÃ´ng gá»™p, index riÃªng fields)
3. BM25 scoring vá»›i field weights
4. Apply filters as query clauses (NOT as post-processing)
5. Return results
```

**Search flow**:
```python
def search(query, filters):
    es_query = {
        "bool": {
            "must": [
                {"multi_match": {"query": query, "fields": ["title^3", "description", "skills^2"]}}
            ],
            "filter": [
                {"term": {"work_type": filters['work_type']}},
                {"term": {"remote_allowed": filters['remote']}},
                {"range": {"salary": {"gte": filters['min_salary'], "lte": filters['max_salary']}}},
            ]
        }
    }
    results = es.search(query=es_query)
    return results
```

**Advantages**:
- âœ… **SIMPLE** - Dá»… hiá»ƒu, dá»… implement
- âœ… Filters work correctly (query-time, not post-processing)
- âœ… BM25 proven to work well for search
- âœ… KhÃ´ng cáº§n ML (fit yÃªu cáº§u project)

**Disadvantages**:
- âš ï¸ KhÃ´ng "fancy" (khÃ´ng cÃ³ embeddings)
- âš ï¸ Cáº§n setup Elasticsearch (hoáº·c implement BM25 custom)

---

## ğŸš€ Káº¾ HOáº CH TRIá»‚N KHAI Má»šI

### Option A: Full Redesign (Approach 1 - Multi-Index)

**Timeline**: 5-7 days

**Day 1: Data pipeline redesign**
- âŒ Bá»: `build_enriched_jobs()` (join táº¥t cáº£ thÃ nh 1 table)
- âœ… Táº O: 
  - `jobs` table (job_id, title, description, company_id, location, salary, work_type, experience, remote)
  - `job_skills` table (job_id, skill_id) - KHÃ”NG gá»™p
  - `job_industries` table (job_id, industry_id) - KHÃ”NG gá»™p
  - `skills` lookup table (skill_id, skill_name)
  - `industries` lookup table (industry_id, industry_name)

**Day 2: Index building**
- Create Index 1: Title + Description (TF-IDF hoáº·c MiniLM)
- Create Index 2: Skills (inverted index hoáº·c embeddings)
- Create Index 3: Metadata (filters)

**Day 3-4: Search implementation**
- Multi-stage retrieval
- Merge & rerank logic
- Filter application (hard constraints)

**Day 5-6: Evaluation & tuning**
- Manual evaluation vá»›i test queries
- Tune weights
- Precision@K, Recall@K (vá»›i manual labels)

**Day 7: UI & documentation**

---

### Option B: Quick Fix (Approach 3 - BM25 + Filters)

**Timeline**: 2-3 days

**Day 1: Switch to BM25**
- Install Whoosh or implement rank_bm25
- Index vá»›i separate fields (title, description, skills)
- Remove vector embeddings (simplify)

**Day 2: Fix filters**
- Filters as query clauses (not post-processing)
- Remove progressive fallback (band-aid)
- Accept 0 results if filters too strict (honest UX)

**Day 3: Cleanup & evaluation**
- Test vá»›i realistic queries
- Document tradeoffs
- Update UI

---

### Option C: Hybrid (Recommended) â­

**Timeline**: 4-5 days

**Day 1: Data pipeline cleanup**
- Keep separate tables (jobs, job_skills, job_industries)
- Create proper indexes with constraints
- Remove aggregation to comma-separated strings

**Day 2: Implement BM25 baseline**
- BM25 vá»›i field weights (title^3, skills^2, description^1)
- Filters as hard constraints (query clauses)
- Measure baseline performance

**Day 3: Add embeddings (optional layer)**
- Keep BM25 as primary
- Add MiniLM embeddings for semantic boost
- Hybrid scoring: 0.7*BM25 + 0.3*Semantic

**Day 4: Evaluation**
- Test suite vá»›i 20 queries (covering different intents)
- Precision@5, @10
- User study (5 people, 5 queries each)

**Day 5: Polish & document**
- UI improvements
- Documentation
- Presentation prep

---

## ğŸ¯ KHUYáº¾N NGHá»Š

**Chá»n Option C: Hybrid Approach**

**LÃ½ do**:
1. âœ… **Realistic timeline**: 4-5 days hoÃ n thÃ nh
2. âœ… **Fix root causes**: Data pipeline Ä‘Ãºng, filters Ä‘Ãºng
3. âœ… **Proven approach**: BM25 + embeddings lÃ  industry standard
4. âœ… **Easy to evaluate**: Precision@K dá»… measure
5. âœ… **Meets requirements**: Full fill project requirements

**Trade-offs**:
- âš ï¸ Bá» progressive fallback â†’ Accept 0 results if filters too strict
- âš ï¸ KhÃ´ng "fancy" nhÆ° pure embeddings â†’ NhÆ°ng work better
- âš ï¸ Cáº§n refactor data pipeline â†’ Worth it

---

## ğŸ“‹ ACTION ITEMS - IMMEDIATE

**Priority 1 - Data Pipeline** (TODAY):
- [ ] Refactor `loader.py`: Keep normalized tables, NO aggregation
- [ ] Create proper data schema (jobs, job_skills, job_industries)
- [ ] Write data validation tests

**Priority 2 - Search Implementation** (DAY 2-3):
- [ ] Implement BM25 (use rank_bm25 library)
- [ ] Rewrite `recommender.py` vá»›i BM25 + filters as clauses
- [ ] Remove progressive fallback (accept 0 results)

**Priority 3 - Evaluation** (DAY 4):
- [ ] Create test query set (20 queries)
- [ ] Manual labeling (relevant/not relevant for each query)
- [ ] Calculate Precision@5, @10

**Priority 4 - Polish** (DAY 5):
- [ ] Update UI
- [ ] Documentation
- [ ] Presentation

---

## ğŸ”š Káº¾T LUáº¬N

**Váº¥n Ä‘á» hiá»‡n táº¡i**: Data pipeline sai tá»« Ä‘áº§u â†’ aggregation máº¥t cáº¥u trÃºc â†’ filters khÃ´ng work â†’ band-aid vá»›i progressive fallback

**Giáº£i phÃ¡p**: Redesign data pipeline â†’ BM25 vá»›i filters Ä‘Ãºng cÃ¡ch â†’ Optional embeddings layer

**Timeline**: 4-5 days vá»›i Option C (Hybrid)

**Next step**: Báº¯t Ä‘áº§u refactor `loader.py` TODAY

---

**Status**: ğŸŸ¡ READY TO START REDESIGN
